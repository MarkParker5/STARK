{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to S.T.A.R.K.","text":""},{"location":"#speech-and-text-algorithmic-recognition-kit","title":"Speech and Text Algorithmic Recognition Kit","text":"<p>Welcome to S.T.A.R.K., a modern, advanced, asynchronous, and fast framework designed for creating intuitive natural language interfaces, especially voice-based. Think of it as the FastAPI but with speech instead of http.</p> <p>New to S.T.A.R.K.? Consider reading the articles in navigation sequentially for a comprehensive understanding of the framework, including the \"Advanced\" section.</p>"},{"location":"#key-features","title":"\ud83d\udd0d Key Features","text":"<ul> <li> <p>\ud83d\udee1\ufe0f Autonomous and Privacy-Focused: Stark operates entirely on-device, ensuring your data remains private. Dive deeper into hosting options here.</p> </li> <li> <p>\ud83e\udde0 Context-Aware: Easily define context and parameters for subsequent requests or parse multiple commands simultaneously. Discover the power of Commands Context.</p> </li> <li> <p>\ud83d\ude80 Asynchronous Commands: Start lengthy tasks and continue using Stark. You'll be notified upon completion. Learn about Sync vs Async Commands and Creating Commands.</p> </li> <li> <p>\ud83d\udcc8 Multiple Responses: Get real-time updates for long tasks, like monitoring download progress. More on this in Creating Commands.</p> </li> <li> <p>\ud83e\udde9 Advanced Patterns Parsing: Our custom patterns syntax makes parsing any parameter from strings effortless.</p> </li> <li> <p>\ud83e\udde0 Extendable with LLMs: Enhance Stark's cognition by integrating leading language models like ChatGPT. More in Fallback Command</p> </li> <li> <p>\ud83c\udf10 Multilingual Support: Interact with Stark in multiple languages.</p> </li> <li> <p>\ud83d\udd27 Absolute Customization: Craft complex commands, integrate various speech or text interfaces, adapt voice assistant modes, or even override existing classes.</p> </li> <li> <p>\ud83c\udf0d Community Support: Join STARK-PLACE repository, the platform library filled with community extensions. Utilize commands crafted by others and share your creations. Further information in Contributing and Shared Usage.</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>The S.T.A.R.K. project is licensed under the CC BY-NC-SA 4.0 International license. You're welcome to modify, contribute to the repository, create, and share forks. Just remember to attribute the original repository and its creator, abstain from commercial use, and retain the existing license.</p> <p>Note: Failing to provide the necessary attribution or using the project for commercial purposes breaches the licensing terms and could have legal consequences.</p>"},{"location":"command-response/","title":"Command Response","text":"<p>The <code>Response</code> class represents the outcome of processing a command in the S.T.A.R.K. This documentation section will help you understand the various properties of the <code>Response</code> class, allowing you to craft detailed and specific responses to user queries.</p>"},{"location":"command-response/#response-properties","title":"Response Properties","text":""},{"location":"command-response/#voice-str","title":"<code>voice: str</code>","text":"<p>Default: <code>''</code></p> <p>This string will be converted to speech and played back to the user. If left empty, no vocal response will be given.</p>"},{"location":"command-response/#text-str","title":"<code>text: str</code>","text":"<p>Default: <code>''</code></p> <p>This property provides a textual representation of the response. It can be displayed in an application interface or used for logging.</p>"},{"location":"command-response/#status-responsestatus","title":"<code>status: ResponseStatus</code>","text":"<p>Default: <code>ResponseStatus.success</code></p> <p>This property indicates the state or result of the command's processing. It can be any of the following values:</p> <ul> <li>none: No status set.</li> <li>not_found: Command not recognized or found.</li> <li>failed: Command processing failed.</li> <li>success: Command processed successfully.</li> <li>info: An informational response.</li> <li>error: An error occurred during command processing.</li> </ul>"},{"location":"command-response/#needs_user_input-bool","title":"<code>needs_user_input: bool</code>","text":"<p>Default: <code>False</code></p> <p>This property, when set to <code>True</code>, signals that the assistant is actively awaiting additional input from the user. Additionally, if the response is queued for repetition and <code>needs_user_input</code> is set to <code>true</code>, the repetition will pause following the current response. This pause gives users the opportunity to address or answer any queries posed by the assistant without being interrupted by subsequent repeated messages.</p>"},{"location":"command-response/#commands-listcommand","title":"<code>commands: list[Command]</code>","text":"<p>Default: <code>[]</code></p> <p>This property contains a list of commands associated with the response. These commands can serve various purposes, such as providing context, suggesting subsequent actions to the user, or even structuring nested menus. It's often beneficial to utilize this in conjunction with the <code>needs_user_input</code> property to create more interactive and guided user experiences.</p>"},{"location":"command-response/#parameters-dictstr-any","title":"<code>parameters: dict[str, Any]</code>","text":"<p>Default: <code>{}</code></p> <p>This property holds a dictionary of supplementary data or context useful to the voice assistant or the underlying command processing framework. Examples include specifying a city when inquiring about the weather or denoting a particular room in the context of smart home operations. This feature enables dynamic and contextual interactions, enhancing the overall user experience.</p>"},{"location":"command-response/#id-uuid","title":"<code>id: UUID</code>","text":"<p>A unique identifier for the response. It gets automatically set when a response is created. For internal usage only.</p>"},{"location":"command-response/#time-datetime","title":"<code>time: datetime</code>","text":"<p>The timestamp when the response was created. It gets automatically set upon the creation of a new response. For internal usage only.</p>"},{"location":"command-response/#repeat_last-response","title":"<code>repeat_last: Response</code>","text":"<p>Static instance of the Response class, that provides a mechanism to reprocess the last given response. If a new response matches the <code>repeat_last</code> instance, the voice assistant will process the previous response again.</p>"},{"location":"command-response/#response-handling-in-the-framework","title":"Response Handling in the Framework","text":"<p>Responses play a vital role in the user interaction flow. The <code>VoiceAssistant</code> class, along with the <code>CommandsContext</code>, processes these responses to ensure the user receives accurate and timely feedback.</p> <ul> <li> <p>Upon receiving a new response: The <code>VoiceAssistant</code> initially verifies if the response status belongs to its ignore list. If it doesn't, the assistant subsequently evaluates the mode's timeout parameters and, if applicable, appends the response to its collection. For further details on this behavior, refer to the Modes section on the VoiceAssistant page.</p> </li> <li> <p>Playing the response: Depending on the assistant's mode, the response may be converted to speech and played back to the user.</p> </li> <li> <p>Repeating responses: If there has been recent interaction, the assistant may opt to repeat specific responses, ensuring the user is reminded of any ongoing processes or required actions.</p> </li> </ul> <p>This dynamic and flexible system of handling responses ensures that the user experience is interactive and engaging.</p> <p>This documentation is meant to provide a concise overview of the <code>Response</code> class and its role within the S.T.A.R.K framework. It's crucial to understand these properties and mechanisms to design a voice assistant that effectively communicates with the user.</p>"},{"location":"commands-context/","title":"Commands Context","text":"<p>The <code>Commands Context</code> feature provides a sophisticated means to manage multi-level command structures. By facilitating a hierarchical command interface, it ensures users enjoy an intuitive and seamless interaction.</p>"},{"location":"commands-context/#managing-multiple-commands","title":"Managing Multiple Commands","text":"<p>In instances where a single input correlates with multiple commands, the system adeptly manages these overlaps. It gives priority to commands based on their position in the string or their declaration sequence, guaranteeing that the most pertinent command always takes precedence.</p>"},{"location":"commands-context/#the-contextual-hierarchy","title":"The Contextual Hierarchy","text":"<p>Visualize the entire system as a tree. Each context functions as a node, with its linked sub-contexts acting as its offspring. As users navigate this tree, they move between nodes\u2014either delving deeper or backtracking\u2014to consistently find the right command match.</p>"},{"location":"commands-context/#command-context-processing","title":"Command Context Processing","text":"<p>When processing a string:</p> <ul> <li>The system adds the root context if it's missing.</li> <li>It checks the current context to find a command that matches the input string. If a command doesn't fit the current context, the system goes up, removing contexts until it finds a match or runs out of contexts.</li> <li>Upon a successful match, the system updates parameters, organizes dependencies, and initiates the command.</li> <li>Unneeded contexts are quickly removed.</li> </ul>"},{"location":"commands-context/#managing-responses","title":"Managing Responses","text":"<p>Responses are neatly lined up. The system constantly checks this line, running responses and commands in the order they come in, ensuring fast and orderly processing.</p>"},{"location":"commands-context/#response-embedded-context","title":"Response-embedded Context","text":"<p>Responses can include:</p> <ul> <li><code>needs_user_input: bool</code>: If set to true, the system halts processing after the current response.</li> <li><code>commands: list[Command]</code>: Commands that can reshape context, propose subsequent actions, or establish layered interfaces.</li> <li><code>parameters: dict[str, Any]</code>: A supporting data list important for later processing or context definition.</li> </ul> <p>For additional details on responses, visit the Command Response page.</p>"},{"location":"commands-context/#code-implementation","title":"Code Implementation","text":"<pre><code>@manager.new('hello', hidden=True) \ndef hello_context(**params):\n    voice = text = f'Hi, {params[\"name\"]}!'\n    return Response(text=text, voice=voice)\n\n@manager.new('bye', hidden=True)\ndef bye_context(name: Word, handler: ResponseHandler):\n    handler.pop_context()\n    return Response(text=f'Bye, {name}!')\n\n@manager.new('hello $name:Word')\ndef hello(name: Word):\n    text = voice = f'Hello, {name}!'\n    return Response(\n        text=text,\n        voice=voice,\n        commands=[hello_context, bye_context],\n        parameters={'name': name}\n    )\n</code></pre> <p>The code example provided demonstrates how to define and manage commands using a fictional <code>manager</code> object.</p>"},{"location":"commands-context/#hello_context-function","title":"<code>hello_context</code> Function","text":"<ul> <li>This function is marked with a <code>hidden=True</code> parameter in its decorator. This means that the command will not be available in the root context, making it inaccessible as a top-level command.</li> <li>The function accepts all context parameters through <code>**params</code>, which is a dictionary.</li> <li>Within the function, both the <code>voice</code> and <code>text</code> variables are set to greet the user, using the context <code>name</code> parameter.</li> <li>It then returns a response with the generated greeting text and voice.</li> </ul>"},{"location":"commands-context/#bye_context-function","title":"<code>bye_context</code> Function","text":"<ul> <li>Similarly, this function is also hidden from the root context.</li> <li>The function accepts specific parameters: <code>name</code> and <code>handler</code>. It's important to note that there's no <code>name</code> in the command pattern, which implies that it must be derived from the context.</li> <li>The <code>handler.pop_context()</code> method is called, which presumably removes the current context, signaling a transition or end of interaction.</li> <li>A farewell response using the <code>name</code> parameter is returned.</li> </ul>"},{"location":"commands-context/#hello-function","title":"<code>hello</code> Function","text":"<ul> <li>This function defines a command pattern where a name is expected as input, formatted as <code>hello $name:Word</code>.</li> <li>Inside, it constructs a greeting using the provided name.</li> <li>The response not only contains the greeting but also a list of commands (<code>hello_context</code> and <code>bye_context</code>) that can be triggered next. This showcases the hierarchical and contextual nature of the system. Additionally, the name is passed as a parameter for potential use in subsequent commands.</li> </ul> <p>In summary, the code example gives us a glimpse into the contextual and hierarchical command management system. With the use of the <code>hidden</code> attribute, commands can be kept away from the root context, making them accessible only when they are contextually relevant.</p>"},{"location":"contributing-and-shared-usage-stark-place/","title":"Contributing and Shared Usage: S.T.A.R.K P.L.A.C.E","text":""},{"location":"contributing-and-shared-usage-stark-place/#stark-platform-library-and-community-extensions","title":"STARK Platform Library and Community Extensions","text":"<p>Stark-Place serves as a repository filled with commands, implementations of various protocols (like speech interfaces), and other extensions that enhance the capabilities of the Stark framework. These features are systematically structured into modules, categorized based on their functionality.</p>"},{"location":"contributing-and-shared-usage-stark-place/#using-stark-place","title":"\ud83d\udce6 Using Stark-Place","text":"<p>To integrate features from Stark-Place into your projects:</p> <p>1. Install it as you would with any pip module.</p> <pre><code>pip install stark-place\n</code></pre> <p>2. Import the <code>general_manager</code> for access to all commands or the specific manager from a module, or any other feature you require.</p> <pre><code>from stark_place.commands import general_manager\n</code></pre>"},{"location":"contributing-and-shared-usage-stark-place/#contributing-to-stark-place","title":"\ud83e\udd1d Contributing to Stark-Place","text":"<p>We welcome and appreciate contributions from the community! Here's how you can contribute:</p> <ol> <li>Fork the Repository: Start by creating a fork of the MarkParker5/STARK-PLACE repository.</li> <li>Optional Branch Creation: If you prefer, you can create a branch within your fork to manage your changes.</li> <li>Add Commands or Features: Either add commands to an existing module or create a new module.</li> <li>Push Your Changes: Once you're satisfied with your additions or modifications, push them to your fork.</li> <li>Open a Pull Request: Finally, head over to the main STARK-PLACE repository and open a pull request. We'll review your contributions and merge them!</li> </ol>"},{"location":"contributing-and-shared-usage-stark-place/#license","title":"License","text":"<p>The Stark-Place project is licensed under the CC BY-NC-SA 4.0 International license. You're welcome to modify, contribute to the repository, create, and share forks. Just remember to attribute the original repository and its creator, abstain from commercial use, and retain the existing license.</p> <p>Note: Failing to provide the attribution or using the project for commercial purposes breaches the licensing terms and could have legal consequences.</p> <p>We're thrilled to have you as part of our community, and we're excited to see the innovative extensions you'll bring to Stark-Place! Remember, every contribution, big or small, helps in shaping Stark-Place into a powerful platform for all Stark users. Join the community, share your expertise, and let's build together!</p>"},{"location":"creating-commands/","title":"Creating Commands","text":"<p>Commands serve as foundational building blocks designed to execute specific actions. They can be implemented either synchronously or asynchronously. In the following sections, we'll explore the specific features of each type and their differences.</p>"},{"location":"creating-commands/#sync-commands","title":"Sync Commands","text":""},{"location":"creating-commands/#simple-command-with-return","title":"Simple Command with <code>return</code>","text":"<p>A synchronous command can straightforwardly return a response, as demonstrated below:</p> <pre><code>from stark import Response, CommandsManager\n\nmanager = CommandsManager()\n\n@manager.new('hello')\ndef hello_command() -&gt; Response:\n    text = voice = 'Hello, world!'\n    return Response(text=text, voice=voice)\n</code></pre>"},{"location":"creating-commands/#multiple-responses-using-yield","title":"Multiple responses using <code>yield</code>","text":"<p>Although it's possible to yield multiple responses in synchronous functions, doing so may block the main thread. This can result in warnings or even halt the application. For multiple responses in sync functions, consider using the <code>ResponseHandler.respond</code> method or contemplate migrating to the async option.</p> <pre><code>@manager.new('foo')\ndef foo() -&gt; Response:\n    yield Response(text='Hello')\n    yield Response(text='World')\n    # more yields...\n</code></pre>"},{"location":"creating-commands/#multiple-responses-using-responsehandlerrespond","title":"Multiple responses using <code>ResponseHandler.respond</code>","text":"<p>To manage multiple responses, the <code>ResponseHandler</code> can be leveraged. Simply include a property of type <code>ResponseHandler</code>, and the dependency injection mechanism will handle it automatically.</p> <pre><code>@manager.new('foo')\ndef foo(handler: ResponseHandler):\n    handler.respond(Response(text='Starting task'))\n    # some processing\n    handler.respond(Response(text='Task progress is 50%'))\n    ...\n    handler.respond(Response(text='Task is done'))\n</code></pre>"},{"location":"creating-commands/#remove-response-using-responsehandlerunrespond","title":"Remove response using <code>ResponseHandler.unrespond</code>","text":"<p>To remove a response, use the <code>unrespond</code> method. If the voice assistant is in waiting mode, the response won't be repeated in the subsequent interaction. Learn more about modes in Voice Assistant.</p> <pre><code>@manager.new('foo')\ndef foo(handler: ResponseHandler):\n    handler.respond(Response(text='Starting task'))\n    ...\n    error = Response(text='No internet connection, retrying task...')\n    handler.respond(error)\n    ...\n    # when the internet connection is restored\n    handler.unrespond(error)\n    handler.respond(Response(text='Task is done'))\n</code></pre>"},{"location":"creating-commands/#call-command-from-another-command","title":"Call command from another command","text":"<p>Commands are inherently async, so we need to syncify the async foo (or declare the current function as async and await <code>foo</code>, see Sync vs Async)</p>"},{"location":"creating-commands/#simple","title":"Simple","text":"<pre><code>from asyncer import syncify\n...\n\n@manager.new('foo')\ndef foo() -&gt; Response:\n    return Response(text='Hello!')\n\n@manager.new('bar')\ndef bar() -&gt; Response: \n    sync_foo = syncify(foo)\n    return sync_foo()\n</code></pre>"},{"location":"creating-commands/#with-dependency-injection","title":"With dependency injection","text":"<p>Include the <code>inject_dependencies</code> property in the function declaration. This function wraps the command for smooth dependency injection. Learn more about dependencies at DI Container.</p> <pre><code>@manager.new('bar')\ndef bar(inject_dependencies): \n    return syncify(inject_dependencies(foo))()\n</code></pre>"},{"location":"creating-commands/#async-commands","title":"Async Commands","text":"<p>Asynchronous commands resemble their synchronous counterparts but offer enhanced features like <code>await</code> and <code>yield</code>.</p>"},{"location":"creating-commands/#simple-command-with-return_1","title":"Simple Command with <code>return</code>","text":"<p>An asynchronous command can effortlessly return a response:</p> <pre><code>@manager.new('hello')\nasync def hello_command() -&gt; Response:\n    text = voice = 'Hello, world!'\n    return Response(text=text, voice=voice)\n</code></pre>"},{"location":"creating-commands/#multiple-responses-using-yield_1","title":"Multiple responses using <code>yield</code>","text":"<p>Yielding multiple responses in asynchronous functions is seamless and doesn't block the main thread.</p> <pre><code>@manager.new('foo')\nasync def foo() -&gt; Response:\n    yield Response(text='Starting task')\n    # some processing\n    yield Response(text='Task progress is 50%')\n    ...\n    yield Response(text='Task is done')\n</code></pre>"},{"location":"creating-commands/#multiple-responses-using-responsehandlerrespond_1","title":"Multiple responses using <code>ResponseHandler.respond</code>","text":"<p>As an alternative to <code>yield</code>, the asynchronous version of <code>ResponseHandler</code>, named <code>AsyncResponseHandler</code>, can be used.</p> <pre><code>@manager.new('foo')\nasync def foo(handler: AsyncResponseHandler):\n    await handler.respond(Response(text='Starting task'))\n    # some processing\n    await handler.respond(Response(text='Task progress is 50%'))\n    ...\n    await handler.respond(Response(text='Task is done'))\n</code></pre>"},{"location":"creating-commands/#remove-response-using-responsehandlerunrespond_1","title":"Remove response using <code>ResponseHandler.unrespond</code>","text":"<p>To remove a response, use the <code>unrespond</code> method. If the voice assistant is in waiting mode, the response won't be repeated in the subsequent interaction. Learn more about modes in Voice Assistant.</p> <pre><code>@manager.new('foo')\nasync def foo(handler: AsyncResponseHandler):\n    await handler.respond(Response(text='Starting task'))\n    ...\n    error = Response(text='No internet connection, retrying task...')\n    await handler.respond(error)\n    ...\n    # once the internet connection is restored\n    await handler.unrespond(error)\n    await handler.respond(Response(text='Task is done'))\n</code></pre> <p>Do note that you can delete responses sent using <code>yield</code> in the same manner. There's no distinction between the two.</p>"},{"location":"creating-commands/#call-command-from-another-command_1","title":"Call command from another command","text":"<p>Commands can be invoked as if they were standard async functions (coroutines).</p>"},{"location":"creating-commands/#simple_1","title":"Simple","text":"<pre><code>@manager.new('foo')\nasync def foo() -&gt; Response:\n    return Response(text='Hello!')\n\n@manager.new('bar')\nasync def bar():\n    return await foo()\n</code></pre>"},{"location":"creating-commands/#with-dependency-injection_1","title":"With dependency injection","text":"<p>For commands with dependencies, the <code>inject_dependencies</code> wrapper ensures seamless injection.</p> <pre><code>@manager.new('foo')\nasync def foo(handler: AsyncResponseHandler) -&gt; Response:\n    handler.respond(Response(text='Hello!'))\n\n@manager.new('bar')\nasync def bar(inject_dependencies): \n    return await inject_dependencies(foo)()\n</code></pre>"},{"location":"creating-commands/#extendingmerging-commands-managers","title":"Extending/merging commands managers","text":"<p>Command managers can be expanded by merging child managers into them.</p> <pre><code>root_manager = CommandsManager()\nchild_manager = CommandsManager('Child')\n\n@root_manager.new('test')\ndef test(): pass\n\n@child_manager.new('test2')\ndef test2(): pass\n\nroot_manager.extend(child_manager) # now root_manager has all commands of child_manager\n</code></pre> <p>In conclusion, the foundational concepts remain consistent whether you employ synchronous or asynchronous commands. The primary distinction is in task handling: asynchronous commands facilitate non-blocking execution. As always, opt for the approach that best aligns with your application's specific requirements.</p>"},{"location":"default-speech-interfaces/","title":"Default Speech Interfaces","text":"<p>The Stark framework offers a default mechanism to incorporate speech interfaces from various platforms. This page elucidates the structure and usage of these interfaces.</p>"},{"location":"default-speech-interfaces/#overview","title":"Overview","text":"<p>Stark's speech interfaces comprise two primary components:</p> <ol> <li>Speech Recognizers: Convert spoken words into text.</li> <li>Speech Synthesizers: Translate text into audible speech.</li> </ol> <p>Both components employ protocols, ensuring flexibility and extensibility when opting for different implementations.</p>"},{"location":"default-speech-interfaces/#voskspeechrecognizer","title":"VoskSpeechRecognizer","text":"<p>An implementation utilizing the Vosk library. This recognizer captures audio input and processes it via the Vosk offline speech recognition engine.</p> <pre><code>def __init__(self, model_url: str):\n</code></pre>"},{"location":"default-speech-interfaces/#silerospeechsynthesizer","title":"SileroSpeechSynthesizer","text":"<p>Implemented using Silero models. The resultant speech can be audibly played using the <code>Speech</code> class's <code>play()</code> method.</p> <pre><code>def __init__(self, model_url: str, speaker: str = 'baya', threads: int = 4, device ='cpu', torch_backends_quantized_engine: str = 'qnnpack'):\n</code></pre>"},{"location":"default-speech-interfaces/#gcloudspeechsynthesizer","title":"GCloudSpeechSynthesizer","text":"<p>This synthesizer leverages Google Cloud's Text-to-Speech service. Ensure your credentials are properly configured before usage. The synthesized speech can be stored as a file for subsequent playback.</p> <pre><code>def __init__(self, voice_name: str, language_code: str, json_key_path: str):\n</code></pre>"},{"location":"default-speech-interfaces/#usage","title":"Usage","text":"<p>To integrate the speech interfaces:</p> <ol> <li>Instantiate <code>CommandsManager</code>.</li> <li>Select and instantiate your preferred speech recognizer.</li> <li>Select and instantiate your preferred speech synthesizer.</li> <li>Deploy the <code>run()</code> function, supplying it with the <code>CommandsManager</code>, recognizer, and synthesizer instances.</li> </ol>"},{"location":"default-speech-interfaces/#example","title":"Example","text":"<pre><code>manager = CommandsManager(...)\nrecognizer = VoskSpeechRecognizer(model_url=\"...\")\nsynthesizer = SileroSpeechSynthesizer(model_url=\"...\")\n\nawait run(manager, recognizer, synthesizer)\n</code></pre> <p>With the above configuration, your application will commence voice command listening and generate synthesized speech based on the logic within the commands manager.</p>"},{"location":"default-speech-interfaces/#notes","title":"Notes","text":"<ol> <li>Confirm the required dependencies, such as Vosk, Silero, and Google Cloud, are in place (refer to Installation).</li> <li>Adequate error management and model verifications are essential for a production environment.</li> <li>For more nuanced interactions based on speech recognition outcomes, adjust the delegates.</li> </ol> <p>Harness Stark's default speech interfaces to effortlessly and flexibly craft voice-centric applications. Choose the most suitable recognizer and synthesizer for your requirements, and integrate them smoothly.</p>"},{"location":"default-speech-interfaces/#implementing-custom-speech-interface","title":"Implementing Custom Speech Interface","text":"<p>For more information, consult Custom Speech Interfaces under the \"Advanced\" section.</p>"},{"location":"dependency-injection/","title":"Dependency Injection","text":"<p>Dependency Injection (DI) is a powerful design pattern used to achieve Inversion of Control (IoC) between classes and their dependencies. Within the context of our voice assistant, Dependency Injection facilitates the provision of specific objects or values to command functions. This ensures that these functions can readily access external resources or other system components.</p> <p>This guide provides an overview of the Dependency Injection implementation, how to utilize it in your voice assistant, and some native dependencies.</p>"},{"location":"dependency-injection/#response-handler","title":"Response Handler","text":"<p>There are two response handlers: <code>AsyncResponseHandler</code> and <code>ResponseHandler</code>. They oversee the processing of responses, asynchronously and synchronously, respectively. To employ them, simply include the required type (class) annotation as an argument within the function declaration. The argument's name isn't significant for this dependency.</p> <pre><code>@manager.new('hello')\nasync def hello(handler: AsyncResponseHandler) -&gt; Response: \n    await handler.respond(Response(text = 'Hi'))\n</code></pre> <p>In the showcased example, the <code>AsyncResponseHandler</code> is automatically injected into the <code>foo</code> command function upon its invocation.</p>"},{"location":"dependency-injection/#inject_dependency","title":"<code>inject_dependency</code>","text":"<p>The <code>inject_dependency</code> method serves to integrate specific dependencies into a function. This method determines the function's dependencies and subsequently calls it. Contrary to the response handler, this dependency is identified by the argument's name.</p> <p>Example: <pre><code>@manager.new('foo')\nasync def foo(handler: AsyncResponseHandler) -&gt; Response: \n    return Response(text = 'foo!')\n\n@manager.new('bar')\nasync def bar(inject_dependencies): \n    return await inject_dependencies(foo)()\n</code></pre></p> <p>Here, the <code>foo</code> dependency is injected and executed within the <code>bar</code> command function.</p>"},{"location":"dependency-injection/#accessing-dicontainer-in-a-command","title":"Accessing DIContainer in a Command","text":"<p>The <code>CommandsContext</code> class initializes with a <code>dependency_manager</code> of the <code>DependencyManager</code> type. This manager undertakes the role of identifying and injecting the requisite dependencies for command functions.</p> <p>To tap into the DIContainer inside a command, simply declare the needed dependency as a command function parameter. The <code>DependencyManager</code> will resolve this parameter and supply the appropriate object or value.</p> <p>For more advanced access, you can extract the container as a dependency of type <code>DIContainer</code>, as demonstrated:</p> <pre><code>@manager.new('baz')\nasync def baz(di_container: DIContainer): \n    di_container.add_dependency(...)\n    di_container.find(...)\n</code></pre> <p>This is feasible because the default DI container internally registers itself as a dependency:</p> <pre><code>default_dependency_manager.add_dependency(None, DependencyManager, default_dependency_manager)\n</code></pre>"},{"location":"dependency-injection/#adding-custom-dependency","title":"Adding Custom Dependency","text":"<p>You can incorporate custom dependencies using the <code>add_dependency</code> method of the default shared instance of <code>DependencyManager</code>.</p> <p>Example: <pre><code>from stark.general.dependencies import default_dependency_manager\n...\ndefault_dependency_manager.add_dependency(\"custom_name\", CustomType, custom_value)\n</code></pre></p> <p>In this instance, a new dependency named <code>custom_name</code>, of <code>CustomType</code>, with the value <code>custom_value</code> is appended. If the name is set to <code>None</code>, you can later choose any name for the function argument; the dependency will be discerned solely by type (like <code>ResponseHandler</code> and <code>AsyncResponseHandler</code>). Conversely, setting the type to <code>None</code> allows the dependency to be detected purely by the argument name (like <code>inject_dependencies</code>).</p>"},{"location":"dependency-injection/#creating-a-custom-container","title":"Creating a Custom Container","text":"<p>To employ a custom container for Dependency Injection in lieu of the default one, instantiate a new <code>DependencyManager</code> and input your custom dependencies. This tailored container can subsequently be utilized during the <code>CommandsContext</code> initialization.</p> <p>Example: <pre><code>custom_dependency_manager = DependencyManager()\ncustom_dependency_manager.add_dependency(...)\n\ncontext = CommandsContext(..., dependency_manager=custom_dependency_manager)\n</code></pre></p> <p>It's worth noting that the CommandsContext always registers several native dependencies upon initialization:</p> <pre><code>self.dependency_manager.add_dependency(None, AsyncResponseHandler, self)\nself.dependency_manager.add_dependency(None, ResponseHandler, SyncResponseHandler(self))\nself.dependency_manager.add_dependency('inject_dependencies', None, self.inject_dependencies)\n</code></pre> <p>However, other native dependencies will be absent in the custom container unless you manually incorporate them.</p> <p>The adaptability provided by the Dependency Injection framework ensures your command functions remain modular, simplifying testing. As you further develop your voice assistant, utilize this system to adeptly handle your dependencies.</p>"},{"location":"first-steps/","title":"First Steps","text":"<p>Congratulations on installing the STARK framework! This guide is designed to help you familiarize yourself with its primary components and to set up your first voice-driven application using STARK. We'll demonstrate how to create a basic voice assistant that responds to the \"hello\" command.</p>"},{"location":"first-steps/#hello-world","title":"Hello World","text":"<p>STARK provides flexibility by allowing you to integrate different implementations for speech recognition and synthesis. For this tutorial, we will employ the Vosk implementation for speech recognition and the Silero implementation for speech synthesis.</p> <p>Before diving in, you'll need to specify URLs for the models. Both Vosk and Silero are designed to automatically download and cache the models upon their first use.</p> <ul> <li>Vosk Model URL: Visit Vosk models to select an appropriate model.</li> <li>Silero Model URL: Visit Silero models to identify a suitable model.</li> </ul> <p>At the heart of STARK is the <code>CommandsManager</code>, a component dedicated to managing the commands your voice assistant can comprehend. Here's a comprehensive example showcasing how to define a new command, initialize the speech recognizer and synthesizer, and run the voice assistant:</p> <pre><code>import anyio\nfrom stark import run, CommandsManager, Response\nfrom stark.interfaces.vosk import VoskSpeechRecognizer\nfrom stark.interfaces.silero import SileroSpeechSynthesizer\n\n\nVOSK_MODEL_URL = \"YOUR_CHOSEN_VOSK_MODEL_URL\"\nSILERO_MODEL_URL = \"YOUR_CHOSEN_SILERO_MODEL_URL\"\n\nrecognizer = VoskSpeechRecognizer(model_url=VOSK_MODEL_URL)\nsynthesizer = SileroSpeechSynthesizer(model_url=SILERO_MODEL_URL)\n\nmanager = CommandsManager()\n\n@manager.new('hello')\nasync def hello_command() -&gt; Response:\n    text = voice = 'Hello, world!'\n    return Response(text=text, voice=voice)\n\nasync def main():\n    await run(manager, recognizer, synthesizer)\n\nif __name__ == '__main__':\n    anyio.run(main)\n</code></pre> <p>In this code snippet, we defined a new command for the voice assistant. When the word \"hello\" is spoken, the <code>hello_command</code> function is triggered, which then issues a greeting in response.</p> <p>It's important to note that STARK accommodates both synchronous (<code>def</code>) and asynchronous (<code>async def</code>) command definitions. For a deeper dive into the use-cases and distinctions between these two command types, consult the Sync vs Async Commands article.</p>"},{"location":"installation/","title":"Installation","text":"<p>This guide will walk you through the installation of the STARK framework and its associated extras. You can use either pip or poetry for the installation. Let's dive right in!</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have Python 3.12 or newer installed. You can verify this with: <pre><code>python --version\n</code></pre></p> <p>On some systems, you may need to use the <code>python3</code> command instead of <code>python</code>: <pre><code>python3 --version\n</code></pre></p>"},{"location":"installation/#avaiable-extras","title":"Avaiable Extras","text":"<p>The STARK framework offers several extras, which are default implementations for its protocols, to facilitate integration with various tools. These extras include:</p> <ul> <li>all: Installs all default implementations. Recommended if you're not well-versed in dependency management.</li> <li>vosk: Vosk (offline speech recognition) implementation of SpeechRecognizer protocol.</li> <li>gcloud: Google Cloud Text-to-Speech implementation of SpeechSynthesizer protocol.</li> <li>silero: Silero Models (offline) implementation of SpeechSynthesizer.</li> <li>sound: Required utilities for processing sound: <code>sounddevice</code> and <code>soundfile</code>.</li> </ul>"},{"location":"installation/#installation-with-pip","title":"Installation with pip","text":"<p>To install the base version of STARK:</p> <pre><code>pip install stark-engine\n</code></pre> <p>To install any of the extras:</p> <pre><code>pip install stark-engine[all]\npip install stark-engine[gcloud]\npip install stark-engine[vosk]\npip install stark-engine[silero]\npip install stark-engine[sound]\n</code></pre> <p>If you encounter the error <code>zsh: no matches found</code>, simply enclose the package name in quotes:</p> <pre><code>pip install \"stark-engine[all]\"\npip install \"stark-engine[gcloud]\"\npip install \"stark-engine[vosk]\"\npip install \"stark-engine[silero]\"\npip install \"stark-engine[sound]\"\n</code></pre>"},{"location":"installation/#installation-with-poetry","title":"Installation with poetry","text":"<p>If you, like me, prefer using poetry to manage dependencies along with a virtual environment, simply replace <code>pip install</code> with <code>poetry add</code>.</p> <pre><code>poetry add stark-engine\npoetry add stark-engine[all]\npoetry add stark-engine[gcloud]\npoetry add stark-engine[vosk]\npoetry add stark-engine[silero]\npoetry add stark-engine[sound]\n</code></pre> <p>If you encounter the error <code>zsh: no matches found</code>, simply enclose the package name in quotes:</p> <pre><code>poetry add \"stark-engine[all]\"\npoetry add \"stark-engine[gcloud]\"\npoetry add \"stark-engine[vosk]\"\npoetry add \"stark-engine[silero]\"\npoetry add \"stark-engine[sound]\"\n</code></pre> <p>With the STARK framework installed and the desired extras in place, you're all set to develop powerful voice-driven applications. Dive into the documentation, experiment, and build great things!</p>"},{"location":"patterns/","title":"Patterns","text":"<p>Patterns in the S.T.A.R.K toolkit are designed to be dynamic and extensible. They are at the core of how custom voice assistants interpret input and match it to commands. This documentation is a comprehensive guide to understanding and working with patterns in S.T.A.R.K.</p>"},{"location":"patterns/#pattern-syntax","title":"Pattern Syntax","text":"<p>At its essence, a pattern is a string that defines the structure of input it should match. The pattern syntax is enriched with special characters and sequences to help it match a variety of inputs dynamically.</p>"},{"location":"patterns/#basics","title":"Basics","text":"<ul> <li><code>**</code>: Matches any sequence of words.</li> <li><code>*</code>: Matches any single word.</li> <li><code>$name:Type</code>: Defines a named parameter of a specific type.</li> </ul> <p>Example: For instance, the pattern <code>'Some ** here'</code> will match both <code>'Some text here'</code> and <code>'Some lorem ipsum dolor here'</code>.</p>"},{"location":"patterns/#advanced-syntax","title":"Advanced Syntax","text":"<p>Selections Selections provide flexibility in your voice command patterns by allowing multiple possibilities for a single command spot. This can be particularly useful in accommodating various ways users might phrase the same request.</p> <ul> <li> <p><code>(foo|bar|baz)</code>: This pattern matches any single option among the three. So, it will match either <code>'foo'</code>, <code>'bar'</code>, or <code>'baz'</code>. Think of it as an \"OR\" choice for the user.</p> </li> <li> <p><code>(foo|bar)?</code>: This pattern introduces an optional choice. It can match <code>'foo'</code>, <code>'bar'</code>, or neither. The <code>?</code> denotes that the preceding pattern (in this case, the choice between <code>'foo'</code> or <code>'bar'</code>) is optional.</p> </li> <li> <p><code>{foo|bar}</code>: This pattern is designed to capture repetitions. It matches one or more occurrences of <code>'foo'</code> or <code>'bar'</code>. For example, if a user said \"foo foo bar\", this pattern would successfully match. Note: Be cautious with this pattern as it can match long, unexpected repetitions.</p> </li> </ul> <p>General Tip: While creating patterns, always keep the user's natural way of speaking in mind. Testing your patterns with real users can help ensure that your voice assistant responds effectively to a variety of commands.</p>"},{"location":"patterns/#parameters-parsing","title":"Parameters Parsing","text":"<p>Voice commands can be dynamic, meaning they can accommodate varying inputs. This is achieved using named parameters in the command pattern, with the <code>$name:Type</code> syntax. When a user input matches a pattern with named parameters, the assistant extracts these parameters and passes them to the corresponding function.</p> <p>For example, consider the pattern <code>'Hello $name:Word'</code>. If a user says, <code>'Hello Stark'</code>, the system will extract a parameter named <code>'name'</code> with the value <code>'Stark'</code>.</p> <p>However, ensure that the function declaration tied to a command pattern includes all the parameters defined in that pattern, using the same names and types. If this isn't done, you'll encounter an exception during command creation.</p> <p>Here's an example:</p> <pre><code>from stark.core.types import Word\n\n@manager.new('Hello $name:Word')\nasync def example_function(name: Word) -&gt; Response:\n    text = voice = f'You said {name}!'\n    return Response(text=text, voice=voice)\n</code></pre>"},{"location":"patterns/#native-types-list","title":"Native Types List","text":"<p>Out of the box, the S.T.A.R.K. comes with native types that can be used as parameter types in patterns. The currently supported native types include:</p> <ul> <li><code>String</code>: Matches any sequence of words (**).</li> <li><code>Word</code>: Matches a single word (*).</li> </ul> <p>It's also worth noting that you can extend the list of types by defining custom object types, as we'll discuss in the next section.</p>"},{"location":"patterns/#defining-custom-object-types","title":"Defining Custom Object Types","text":"<p>The S.T.A.R.K toolkit isn't just limited to native types; it empowers developers to define their own custom object types. These bespoke types are constructed by subclassing the <code>Object</code> base class and specifying a distinct matching pattern.</p> <p>A standout feature of the S.T.A.R.K toolkit's patterns is their seamless compatibility with nested objects. In essence, a custom object type can house parameters that are, in themselves, other custom object types. This nesting capability facilitates the crafting of complex and nuanced patterns, capable of interpreting diverse input configurations.</p> <p>Below is a demonstrative example of how one might structure a custom object type:</p> <pre><code>class FullName(Object):\n    first_name: Word\n    second_name: Word\n\n    @classproperty\n    def pattern(cls) -&gt; Pattern:\n        return Pattern('$first_name:Word $second_name:Word')\n\ncontext = CommandsContext(...)\ncontext.pattern_parser.register_parameter_type(FullName)\n</code></pre> <p>Upon successfully matching the pattern, S.T.A.R.K will autonomously parse and assign values to <code>first_name</code> and <code>second_name</code>. It's imperative, just as with command patterns, that class properties are congruent with the pattern in terms of both name and type.</p> <p>The section is well-detailed, but I have a few recommendations to make it even clearer:</p>"},{"location":"patterns/#advanced-object-types-with-parsing-customization","title":"Advanced Object Types with Parsing Customization","text":"<p>In instances where the default parsing doesn't cater to your requirements, or when you need specialized processing, the <code>did_parse</code> method comes to the rescue. By overriding this method in custom object types, you can introduce intricate transformations or run custom validation checks post-parsing.</p> <p>Here's an illustrative example:</p> <pre><code>class Lorem(Object):\n\n    @classproperty\n    def pattern(cls):\n        return Pattern('* ipsum')\n\n    async def did_parse(self, from_string: str) -&gt; str:\n        '''\n        Invoked after parsing from the string and assigning the parameters detected in the pattern.\n        Directly calling this method is typically unnecessary and uncommon.\n\n        Override this method to achieve more sophisticated string parsing.\n        '''\n\n        if 'lorem' not in from_string:\n            raise ParseError('lorem not found') # Throw a ParseError if the string doesn't meet certain criteria\n\n        self.value = 'lorem' # Assign additional properties (properties inferred from the pattern are auto-assigned)\n        return 'lorem' # Return the smallest substring essential for this object\n\ncontext = CommandsContext(...)\ncontext.pattern_parser.register_parameter_type(Lorem)\nprint(context.pattern_parser.parse_object(Lorem, \"lorem ipsum\"))\n</code></pre>"},{"location":"patterns/#custom-parser-class-example","title":"Custom Parser Class Example","text":"<p>In some cases, you may want to separate the parsing logic from your data model. This is especially useful when you want to reuse parsing logic, inject dependencies, have longer life cycle, or just keep your models clean. You can define a dedicated parser class for your object type.</p> <p>Here's an example:</p> <pre><code>from stark.core.types import Object, Word\nfrom stark.core.parsing import Pattern, PatternParser, ObjectParser\n\nclass Lorem(Object):\n    @classproperty\n    def pattern(cls):\n        return Pattern(\"* ipsum\")\n\nclass LoremParser(ObjectParser):\n    def __init__(self, pattern_parser: PatternParser):\n        self.pattern_parser = pattern_parser\n\n    async def did_parse(self, obj: Lorem, from_string: str) -&gt; str:\n        # Custom parsing logic for Lorem\n        if \"lorem\" not in from_string:\n            raise ParseError(\"lorem not found\")\n        obj.value = \"lorem\"\n        return \"lorem\"\n\ncontext = CommandsContext(...)\ncontext.pattern_parser.register_parameter_type(Lorem, parser=LoremParser())\nprint(context.pattern_parser.parse_object(Lorem, \"lorem ipsum\"))\n</code></pre> <p>This approach allows you to keep parsing logic separate from your data model and makes it easy to inject dependencies or share logic between different models.</p> <p>Note that the <code>did_parse</code> method must return a substring of the input string that was successfully parsed. This substring should be the smallest possible string that still represents the object's value. In case you use 3rd party parser that can't extract substring and just provides the value, you have several options to handle this:</p> <ol> <li>If your parser returns a string-ish value, like some kind of name, you can use <code>levenshtein_search_substring</code> from the STARK-Levenshtein module. This will allow you efficiently find the closest fuzzy match of your named entity in the input string.</li> <li>Consider using <code>NLDictionaryName</code> from Phonetic Dictionary if suits your needs.</li> <li>If options above are not suitable, take a look at sliding_window_parser wrapper. Note that it will call the parser method multiple times to find the best match, which can be optimized by caching intermediate results inside your parser func, but yet still requires careful usage especially with large input strings and long io-bound parsing times.</li> </ol>"},{"location":"patterns/#recommended-use-of-caching-for-did_parse-method","title":"Recommended Use of Caching for <code>did_parse</code> Method","text":"<p>When the <code>did_parse</code> method is involved in the matching process, especially if it performs complex computations or external lookups, it can slow down the overall matching process. To alleviate this potential bottleneck, it's highly recommended to use caching. By storing previously parsed objects in a cache, you can avoid redundant work and improve the overall performance of your custom voice assistant.</p> <p>By understanding and mastering patterns in the S.T.A.R.K toolkit, you'll be well-equipped to create powerful and dynamic custom voice assistants. Happy coding!</p>"},{"location":"sync-vs-async-commands/","title":"Sync vs Async Commands","text":""},{"location":"sync-vs-async-commands/#tldr","title":"TLDR","text":""},{"location":"sync-vs-async-commands/#needs-await","title":"Needs await","text":"<p>If you're using third-party libraries that require <code>await</code>, such as</p> <pre><code>results = await some_library()\n</code></pre> <p>Declare your command using <code>async def</code>:</p> <pre><code>@manager.new('hello')\nasync def hello_command() -&gt; Response:\n    text = voice = await some_library()  # asynchronous call\n    return Response(text=text, voice=voice)\n</code></pre>"},{"location":"sync-vs-async-commands/#blocking-code","title":"Blocking Code","text":"<p>If your command contains blocking synchronous code (e.g., using the <code>requests</code> library or <code>time.sleep</code>), declare it using <code>def</code>:</p> <pre><code>import requests\n\n@manager.new('hello')\ndef hello_command() -&gt; Response:\n    requests.get('https://stark.markparker.me/')  # synchronous blocking code\n    text = voice = 'Hello, world!'\n    return Response(text=text, voice=voice)\n</code></pre>"},{"location":"sync-vs-async-commands/#only-fast-code","title":"Only Fast Code","text":"<p>For commands that don't need to wait for external responses or perform long computations, you can use both <code>async def</code> and <code>def</code>.</p>"},{"location":"sync-vs-async-commands/#unsure","title":"Unsure?","text":"<p>If you just don't know, use normal <code>def</code>.</p>"},{"location":"sync-vs-async-commands/#mix-of-blocking-and-async","title":"Mix of Blocking and Async","text":"<p>If your command contains both blocking code and <code>await</code>-requiring asynchronous code, you'll need to use asyncer. There are two methods:</p> <p>1. Recommended: Declare the command with <code>async def</code>, use <code>await</code> for asynchronous functions, and wrap blocking code in <code>asyncer.asyncify</code>:</p> <pre><code>import asyncer\nimport requests\n\n@manager.new('hello')\nasync def hello_command() -&gt; Response:\n    await some_library() # asynchronous function\n    await asyncer.asyncify(requests.get)('https://stark.markparker.me/') # converted to asynchronous\n    text = voice = 'Hello, world!'\n    return Response(text=text, voice=voice)\n</code></pre> <p>2. Use a regular <code>def</code> for the command, execute blocking functions as-is, and wrap asynchronous functions in <code>asyncer.syncify</code>:</p> <pre><code>import asyncer\nimport requests\n\n@manager.new('hello')\ndef hello_command() -&gt; Response:\n    asyncer.syncify(some_library)() # converted to synchronous\n    requests.get('https://stark.markparker.me/') # blocking code\n    text = voice = 'Hello, world!'\n    return Response(text=text, voice=voice)\n</code></pre>"},{"location":"sync-vs-async-commands/#technical-details","title":"Technical Details","text":"<p>All commands in Stark are inherently asynchronous. If you declare a command as synchronous, Stark converts it to asynchronous using asyncer.asyncify.</p> <p>By default, Stark concurrently manages two vital processes: speech transcription and response handling. It also has to execute commands, adding temporary processes that last as long as the command. All these processes share a single main thread. If one process blocks the thread for an extended period (e.g., with <code>requests.get</code> or <code>time.sleep</code>), it can halt the entire application. Stark includes the <code>BlockageDetector</code> to monitor the main thread and alert you if it's blocked for longer than a specified duration (default is 1 second).</p> <p>For commands that might cause blockages, declaring them using def is advised. Stark will then wrap these commands with asyncer.asyncify, spawning separate background threads for each process.</p> <p>When using async def, care should be taken to prevent the main thread from being blocked. This can be achieved by avoiding long-blocking code and opting for asynchronous libraries like <code>aiohttp</code> over synchronous ones such as <code>requests</code>. Additionally, <code>asyncer.asyncify</code> can be used to wrap blocking sections of code.</p> <p>For a deeper dive into synchronous vs. asynchronous programming, check FastAPI documentation page. To learn more about transitioning between functions and threads, refer to the asyncer documentation.</p>"},{"location":"voice-assistant/","title":"Voice Assistant (VA) Documentation","text":""},{"location":"voice-assistant/#env-parameters","title":"Env Parameters","text":"<p><code>STARK_VOICE_CLI</code>: Prints voice input and output in terminal if set to 1 (default 0). Useful for testing and debugging if no other interface is available.</p>"},{"location":"voice-assistant/#overview","title":"Overview","text":"<p>The VA processes user speech inputs, interacts with a set of commands, and provides responses. The behavior and response of the VA can be modified by setting different \"modes\". These modes define how the VA should operate in various situations, such as active listening, waiting, or when it's inactive.</p>"},{"location":"voice-assistant/#how-the-va-works","title":"How the VA Works","text":""},{"location":"voice-assistant/#responses-and-contexts-in-different-modes","title":"Responses and Contexts in Different Modes","text":"<p>The VA processes user inputs and responds based on the current context and mode. A context can be thought of as a state or situation in which the VA finds itself. Depending on the mode, the VA might immediately play responses, collect them for later, require explicit triggers to respond, or have different timeouts after which it changes its behavior or mode.</p>"},{"location":"voice-assistant/#effects-of-modes-on-va","title":"Effects of Modes on VA","text":"<p>The mode can change the VA's behavior in various ways, such as:</p> <ul> <li>Whether to immediately play responses.</li> <li>Whether to collect responses for future playbacks.</li> <li>Setting a pattern for explicit interactions.</li> <li>Setting timeouts for interactions or before repeating a response.</li> <li>Switching to another mode either after a timeout or an interaction.</li> <li>Deciding to stop after an interaction.</li> </ul>"},{"location":"voice-assistant/#mode-details","title":"Mode Details","text":"<p>The <code>Mode</code> class defines the behavior and settings of the VA in various situations. Each property of the <code>Mode</code> class influences the VA's interaction with the user and the context.</p>"},{"location":"voice-assistant/#mode-properties","title":"Mode Properties","text":"<ul> <li><code>play_responses: bool</code> (default: <code>True</code>)</li> </ul> <p>Determines whether the VA should immediately play the responses to user inputs. If set to <code>False</code>, the VA might hold onto responses for later or not vocalize them at all, based on other mode settings.</p> <ul> <li><code>collect_responses: bool</code> (default: <code>False</code>)</li> </ul> <p>Indicates if the VA should collect responses for later playback. When set to <code>True</code>, responses might be saved and played back later, especially if <code>play_responses</code> is set to <code>False</code>.</p> <ul> <li><code>explicit_interaction_pattern: Optional[str]</code> (default: <code>None</code>)</li> </ul> <p>This can be set to a specific string pattern. When defined, the VA requires an explicit interaction matching this pattern before processing user input. This is useful for \"wake word\" or command activation scenarios.</p> <ul> <li><code>timeout_after_interaction: int</code> (default: <code>20</code>)</li> </ul> <p>Defines the number of seconds the VA waits after the last interaction before considering the session as timed out. Depending on other mode settings, the VA might change its behavior or switch modes after a timeout.</p> <ul> <li><code>timeout_before_repeat: int</code> (default: <code>5</code>)</li> </ul> <p>Specifies the number of seconds before the VA can repeat a previously played response.</p> <ul> <li><code>mode_on_timeout: Callable[[], Mode] | None</code> (default: <code>None</code>)</li> </ul> <p>Defines a function that returns another mode that the VA should switch to after a timeout.</p> <ul> <li><code>mode_on_interaction: Callable[[], Mode] | None</code> (default: <code>None</code>)</li> </ul> <p>Determines a function that returns another mode that the VA should switch to upon receiving an interaction from the user.</p> <ul> <li><code>stop_after_interaction: bool</code> (default: <code>False</code>)</li> </ul> <p>If set to <code>True</code>, the VA will stop its current operation after the command response. This is useful for situations where you want to start the VA on extarnal triggers, like keyboard shortcut.</p>"},{"location":"voice-assistant/#native-modes","title":"Native Modes","text":"<ul> <li>Active: The VA is in an active listening state, transitioning to the \"waiting\" mode upon timeout.</li> <li>Waiting: The VA collects responses and goes back to the \"active\" mode upon user interaction.</li> <li>Inactive: The VA doesn't immediately play responses but collects them, reverting to \"active\" mode upon interaction.</li> <li>Sleeping: Similar to inactive, but requires an explicit interaction pattern to activate.</li> <li>Explicit: Requires a specific interaction pattern to proceed every command.</li> <li>External: Similar to Explicit, but requires an external trigger to activate.</li> </ul>"},{"location":"voice-assistant/#mode-class-code","title":"Mode Class Code","text":"<pre><code>class Mode(BaseModel):\n\n    play_responses: bool = True\n    collect_responses: bool = False\n    explicit_interaction_pattern: Optional[str] = None\n    timeout_after_interaction: int = 20 # seconds\n    timeout_before_repeat: int = 5 # seconds\n    mode_on_timeout: Callable[[], Mode] | None = None\n    mode_on_interaction: Callable[[], Mode] | None = None\n    stop_after_interaction: bool = False\n\n    @classproperty\n    def active(cls) -&gt; Mode:\n        return Mode(\n            mode_on_timeout = lambda: Mode.waiting,\n        )\n\n    @classproperty\n    def waiting(cls) -&gt; Mode:\n        return Mode(\n            collect_responses = True,\n            mode_on_interaction = lambda: Mode.active,\n        )\n\n    @classproperty\n    def inactive(cls) -&gt; Mode:\n        return Mode(\n            play_responses = False,\n            collect_responses = True,\n            timeout_after_interaction = 0, # start collecting responses immediately\n            timeout_before_repeat = 0, # repeat all\n            mode_on_interaction = lambda: Mode.active,\n        )\n\n    @classmethod\n    def sleeping(cls, pattern: str) -&gt; Mode:\n        return Mode(\n            play_responses = False,\n            collect_responses = True,\n            timeout_after_interaction = 0, # start collecting responses immediately\n            timeout_before_repeat = 0, # repeat all\n            explicit_interaction_pattern = pattern,\n            mode_on_interaction = lambda: Mode.active,\n        )\n\n    @classmethod\n    def explicit(cls, pattern: str) -&gt; Mode:\n        return Mode(\n            explicit_interaction_pattern = pattern,\n        )\n\n    @classmethod\n    def external(cls) -&gt; Mode:\n        return Mode(\n            stop_after_interaction = True,\n        )\n</code></pre>"},{"location":"voice-assistant/#changing-modes-manually","title":"Changing Modes Manually","text":"<p>You can manually set the mode by assigning a Mode object to the VA's <code>mode</code> attribute. For instance, to set the VA to \"waiting\" mode:</p> <pre><code>voice_assistant.mode = Mode.waiting\n</code></pre>"},{"location":"voice-assistant/#setting-up-a-custom-mode","title":"Setting Up a Custom Mode","text":"<p>To define a custom mode, create an instance of the <code>Mode</code> class and specify the desired properties. For example:</p> <pre><code>custom_mode = Mode(play_responses=False, timeout_after_interaction=10)\nvoice_assistant.mode = custom_mode\n</code></pre>"},{"location":"voice-assistant/#setting-va-modes-from-command","title":"Setting VA Modes from Command","text":"<p>To have commands in the VA interact with its modes.</p> <ol> <li> <p>Register VA in DIContainer</p> </li> <li> <p>Add VA as a command dependency</p> </li> <li> <p>Access VA in command</p> </li> </ol> <p>check Dependency Injection for details</p>"},{"location":"voice-assistant/#customizing-va-and-observing-events","title":"Customizing VA and Observing Events","text":"<p>If you want to add a custom logic to VA events, for example update GUI, you can subclass the native VoiceAssistant class and override its methods to add desired behavior. Don't forget to call the superclass method to ensure the default behavior is preserved. Voice assistant conforms to SpeechRecognizerDelegate and CommandsContextDelegate protocols, which methods are the main events.</p> <pre><code>class MyVoiceAssistant(VoiceAssistant):\n\n    async def speech_recognizer_did_receive_final_result(self, result: str):\n        super().speech_recognizer_did_receive_final_result(result)\n        print('You sad: ', result) # Your custom logic here\n\n    async def speech_recognizer_did_receive_partial_result(self, result: str):\n        super().speech_recognizer_did_receive_partial_result(result)\n        print(f\"\\rListening...: \\x1b[3m{result}\\x1b[0m\", end=\"\") # Your custom logic here\n\n    async def speech_recognizer_did_receive_empty_result(self):\n        super().speech_recognizer_did_receive_empty_result()\n        # Your custom logic here\n\n    async def commands_context_did_receive_response(self, response: Response):\n        super().commands_context_did_receive_response(response)\n        print('STARK: ', response.text) # Your custom logic here\n</code></pre> <p>For more advanced usage, see the source code or use your IDE's autocomplete. Most modern editors support \"go to definition\" feature which might be very helpful for this.</p>"},{"location":"where-to-host/","title":"Where to Host","text":"<p>The flexibility of the Python programming language allows Stark to be hosted on virtually any system capable of running a Python interpreter. Here\u2019s a guide on where you can run Stark:</p>"},{"location":"where-to-host/#unix-based-systems-macos-linux","title":"Unix-based Systems (macOS, Linux)","text":"<p>Both macOS and Linux are Unix-based systems that typically come with Python pre-installed. However:</p> <ul> <li> <p>Ensure that your Python version is updated to at least 3.12. If it isn't, consider updating it.</p> </li> <li> <p>If you wish to run Stark on boot and keep it running in the background, you can utilize <code>systemd</code> services to automate this process.</p> </li> </ul>"},{"location":"where-to-host/#windows","title":"Windows","text":"<p>Windows doesn\u2019t come with Python pre-installed, but setting it up is straightforward:</p> <ul> <li> <p>Download and install Python from python.org.</p> </li> <li> <p>Running Stark on Windows presents its set of challenges. If you're looking to give Stark a graphical interface, consider frameworks like PyQt, Tkinter, Edifice, and others.</p> </li> <li> <p>Alternatively, for a more minimalist approach, Stark can be integrated into a system tray program using libraries like pystray or infi.systray, thus enabling a voice-only interface.</p> </li> </ul>"},{"location":"where-to-host/#mobile-platforms","title":"Mobile Platforms","text":""},{"location":"where-to-host/#android","title":"Android","text":"<p>As of now, a direct port of Stark for Android has not been achieved. However, you can potentially make use of the Kivy framework which is designed for building cross-platform apps using Python.</p>"},{"location":"where-to-host/#ios","title":"iOS","text":"<p>An iOS port for Stark is currently under development, with no fixed release date. Similarly to Android, you might find success using the cross-platform Kivy framework.</p>"},{"location":"where-to-host/#raspberry-pi-based-hosting","title":"Raspberry Pi-Based Hosting","text":"<p>The Raspberry Pi, given its versatility and cost-effectiveness, can be a perfect host for Stark. Its compact size, affordability, and wide community support make it an attractive option.</p> <p>To set up Stark on a Raspberry Pi:</p> <ol> <li>Ensure you have a Raspberry Pi with an appropriate operating system installed (e.g., Raspberry Pi OS).</li> <li>Connect a microphone to the Raspberry Pi. If you aim for high voice recognition accuracy, consider using a high-sensitive omnidirectional microphone.</li> <li>Connect a speaker or, as in the shared example, a TV soundbar to the Raspberry Pi for output.</li> <li>Install Python (ensure version 3.12 or later) and other necessary packages for Stark.</li> <li>If you wish to run Stark on boot and keep it running in the background, you can utilize <code>systemd</code> services to automate this process.</li> </ol>"},{"location":"where-to-host/#server-based-hosting","title":"Server-Based Hosting","text":"<p>For those looking for a more robust and scalable solution, server-based hosting offers many benefits, like access to Stark from enywhere via the internet.</p> <ol> <li> <p>VPS Hosting: Virtual Private Servers (VPS) allow you to run Stark on remote servers. This is useful if you need higher computational power, redundancy, or want to ensure that Stark remains operational even if local power or network fails.</p> </li> <li> <p>Home Server: You can host Stark on a dedicated home server or even on personal PCs. This can be a dedicated machine or single-board computers like the Raspberry Pi. The advantage is local access and full control over your data and operations.</p> </li> <li> <p>Custom Interfaces: With Stark running on a server, you can develop custom interfaces for access. For example, by implementing an HTTP server, as was done in the shared example, you can connect other devices to Stark. Detailed instructions can be found at Custom Interfaces.</p> </li> </ol>"},{"location":"where-to-host/#personal-experience","title":"Personal Experience","text":"<p>To offer some inspiration, here's a mixed setup that's been effectively used:</p> <p>Stark was set to run 24/7 on a dedicated Raspberry Pi at home, connected to a high-quality sensitive omnidirectional microphone and a TV soundbar for audio output. An Arduino microphone module was also attached, enabling a double-clap mechanism to wake up Stark.</p> <p>Additionally, a small HTTP server was implemented on the Raspberry Pi, allowing a mobile phone to connect to Stark at home. The native Android libraries handled Speech-to-Text (STT) and Text-to-Speech (TTS) functionalities, and the app communicated with the Raspberry Pi using transcribed text via HTTP.</p> <p>To ensure Stark was accessible from anywhere in the world, ngrok was set up on the Raspberry Pi, creating a secure tunnel to the localhost, making the locally hosted Stark globally accessible.</p> <p>Also, a telegram bot was implemented as an inerface for both voice and text messages, used as an additinal cross-platform remote communication way.</p> <p>Such setups illustrate the flexibility and scalability of Stark. Whether you're working with a Raspberry Pi or a dedicated server, there's room for innovation and customization in how you host and interact with Stark.</p>"},{"location":"where-to-host/#important-note","title":"Important Note","text":"<p>Want to see the various platforms Stark has been adapted for? Visit the STARK-PLACE repository to find implemented ports and extensions. If you\u2019ve developed a unique runner for Stark \u2013 be it tray, GUI, Kivy-based, or any other kind \u2013 consider contributing to the community. Open a PR to STARK-PLACE; let's work together to develop the best VA platform ever, enhancing the user experience for everyone!</p>"},{"location":"advanced/custom-interfaces/","title":"Speech Interface Protocols and Custom Implementation","text":"<p>When working with voice-driven applications, a robust and flexible architecture for handling both speech recognition and synthesis is vital. The Stark framework provides these features via interfaces (protocols) that can be easily extended and customized. This page dives deeper into the Stark framework's speech interface protocols and provides details on their implementation.</p>"},{"location":"advanced/custom-interfaces/#recognizer","title":"Recognizer","text":""},{"location":"advanced/custom-interfaces/#protocol","title":"Protocol","text":"<pre><code>@runtime_checkable\nclass SpeechRecognizerDelegate(Protocol):\n    async def speech_recognizer_did_receive_final_result(self, result: str): pass\n    async def speech_recognizer_did_receive_partial_result(self, result: str): pass\n    async def speech_recognizer_did_receive_empty_result(self): pass\n\n@runtime_checkable\nclass SpeechRecognizer(Protocol):\n    is_recognizing: bool\n    delegate: SpeechRecognizerDelegate | None\n\n    async def start_listening(self): pass\n    def stop_listening(self): pass\n</code></pre>"},{"location":"advanced/custom-interfaces/#explanation","title":"Explanation","text":""},{"location":"advanced/custom-interfaces/#speechrecognizerdelegate","title":"SpeechRecognizerDelegate","text":"<p>This protocol provides callback methods to output results of various states of the speech recognition:</p> <ul> <li><code>speech_recognizer_did_receive_final_result</code>: Triggered when a final transcript is available.</li> <li><code>speech_recognizer_did_receive_partial_result</code>: Fired upon receiving an interim transcript.</li> <li><code>speech_recognizer_did_receive_empty_result</code>: Called when no speech was detected.</li> </ul>"},{"location":"advanced/custom-interfaces/#speechrecognizer","title":"SpeechRecognizer","text":"<p>This protocol defines the primary input interface for any speech recognition implementation. It consists of:</p> <ul> <li><code>is_recognizing</code>: A flag indicating if the recognizer is currently active.</li> <li><code>delegate</code>: An instance responsible for handling the recognition results.</li> <li><code>start_listening</code>: A method to initiate the listening process.</li> <li><code>stop_listening</code>: A method to halt the listening process.</li> </ul>"},{"location":"advanced/custom-interfaces/#implementation-reference","title":"Implementation Reference","text":"<p>To illustrate a custom implementation, we can reference the <code>VoskSpeechRecognizer</code>. This implementation leverages the Vosk offline speech recognition library. It downloads and initializes the Vosk model, sets up an audio queue, and provides methods to start and stop the recognition process.</p> <p>For a deeper understanding, review the source code of the <code>VoskSpeechRecognizer</code> implementation.</p>"},{"location":"advanced/custom-interfaces/#synthesizer","title":"Synthesizer","text":""},{"location":"advanced/custom-interfaces/#protocol_1","title":"Protocol","text":"<pre><code>@runtime_checkable\nclass SpeechSynthesizerResult(Protocol):\n    async def play(self): pass\n\n@runtime_checkable   \nclass SpeechSynthesizer(Protocol):\n    async def synthesize(self, text: str) -&gt; SpeechSynthesizerResult: pass\n</code></pre>"},{"location":"advanced/custom-interfaces/#explanation_1","title":"Explanation","text":"<ul> <li> <p>SpeechSynthesizerResult: This protocol defines a structure for the output of the speech synthesis process. It provides a method, <code>play</code>, to audibly present the synthesized speech.</p> </li> <li> <p>SpeechSynthesizer: This protocol represents the primary interface for any speech synthesis implementation. It contains:</p> </li> <li><code>synthesize</code>: An asynchronous method that takes text input and returns a <code>SpeechSynthesizerResult</code> instance.</li> </ul>"},{"location":"advanced/custom-interfaces/#implementation-reference_1","title":"Implementation Reference","text":"<p>For a hands-on example, the <code>SileroSpeechSynthesizer</code> and <code>GCloudSpeechSynthesizer</code> classes illustrate how one might implement the synthesizer protocol using the Silero models and Google Cloud Text-to-Speech services, respectively.</p> <p>To gain more insights, you can check the source code of the <code>SileroSpeechSynthesizer</code> implementation.</p>"},{"location":"advanced/custom-interfaces/#alternative-interfaces","title":"Alternative Interfaces","text":""},{"location":"advanced/custom-interfaces/#cli-interface","title":"CLI Interface","text":"<p>In this approach, you leverage the terminal or command line of a computer as the interface for both speech recognition and synthesis. Instead of speaking into a microphone and receiving audio feedback:</p> <ul> <li> <p>Recognition: Users type their queries or commands into the terminal. The system then processes these textual inputs as if they were transcribed from spoken words.</p> </li> <li> <p>Synthesis: Instead of \"speaking\" or playing synthesized voice, the system displays the response as text in the terminal. This creates a chat-like experience directly within the terminal.</p> </li> </ul> <p>This is an excellent method for debugging, quick testing, or when dealing with environments where audio interfaces aren't feasible.</p>"},{"location":"advanced/custom-interfaces/#gui-interface","title":"GUI Interface","text":"<p>The GUI (Graphical User Interface) provides an intuitive and interactive way to implement custom speech interfaces for voice assistants. It offers a multifaceted experience, allowing users to:</p> <ul> <li> <p>Text Outputs: Display text-based responses, enabling clear communication with users through written messages.</p> </li> <li> <p>Context Visualization: Visualize context and relevant information using graphics, charts, or interactive elements to enhance user understanding.</p> </li> <li> <p>Text and Speech Input: Accept input through both text and speech, allowing users to interact in the manner most convenient for them.</p> </li> <li> <p>Trigger with Buttons: Incorporate buttons or interactive elements that users can click or tap to initiate voice assistant interactions, providing a user-friendly interface.</p> </li> </ul> <p>The GUI interface serves as a versatile canvas for crafting engaging voice assistant experiences, making it an excellent choice for applications where graphical interaction enhances user engagement and comprehension.</p>"},{"location":"advanced/custom-interfaces/#telegram-bot-as-an-interface","title":"Telegram Bot as an Interface","text":"<p>Telegram, a popular messaging platform, provides an amazing bot API that developers can use to create custom bots. By leveraging this API, you can emulate speech interfaces in two distinct ways:</p>"},{"location":"advanced/custom-interfaces/#1-voice-messages","title":"1. Voice Messages","text":"<ul> <li> <p>Recognition: Users send voice messages to the Telegram bot. These voice messages can be transcribed into text using a speech recognition system. The recognized text can then be processed further by the bot for commands or queries.</p> </li> <li> <p>Synthesis: Instead of sending back text responses, the bot can use a text-to-speech system to generate voice messages, which it then sends back to the users. This method provides a more authentic \"voice assistant\" experience within the messaging environment.</p> </li> </ul> <p>By utilizing voice messages, you can create a more immersive experience for users, closely resembling interactions with traditional voice assistants.</p>"},{"location":"advanced/custom-interfaces/#2-text-messages","title":"2. Text Messages","text":"<ul> <li> <p>Recognition: Users send text messages to the Telegram bot. The bot then treats these messages as if they were the transcribed text of spoken words.</p> </li> <li> <p>Synthesis: Rather than synthesizing spoken responses, the bot sends back text messages as its replies. The users read these messages as if they were listening to the synthesized voice of the system.</p> </li> </ul> <p>This approach offers a chat-like experience directly within the Telegram app, providing a seamless interaction that many users find intuitive.</p> <p>In both methods, the use of a Telegram bot allows developers to introduce voice command functionalities in messaging environments, reaching users on various devices and platforms.</p> <p>Venture in mind that these are mere illustrations of potential implementations. The canvas of possibilities is vast, bounded solely by the horizons of your creativity.</p>"},{"location":"advanced/custom-run/","title":"Custom Run","text":"<p>STARK's flexibility and extensibility can be attributed to its ability to cater to various use cases and environments. An essential feature of the framework is the capacity to customize the run function. This allows developers to personalize the core functionality, integrating custom setups, or extending the capabilities of the framework.</p> <p>Below is a quick guide on how to understand and make use of the custom run function.</p>"},{"location":"advanced/custom-run/#understanding-the-default-run-function","title":"Understanding the Default Run Function","text":"<p>The <code>run</code> function in STARK serves as the primary entry point that sets up and commences the voice assistant.</p> <pre><code>import asyncer\n\nfrom stark.interfaces.protocols import SpeechRecognizer, SpeechSynthesizer\nfrom stark.core import CommandsContext, CommandsManager\nfrom stark.voice_assistant import VoiceAssistant\nfrom stark.general.blockage_detector import BlockageDetector\n\n\nasync def run(\n    manager: CommandsManager,\n    speech_recognizer: SpeechRecognizer,\n    speech_synthesizer: SpeechSynthesizer\n):\n    async with asyncer.create_task_group() as main_task_group:\n        context = CommandsContext(\n            task_group = main_task_group, \n            commands_manager = manager\n        )\n        voice_assistant = VoiceAssistant(\n            speech_recognizer = speech_recognizer,\n            speech_synthesizer = speech_synthesizer,\n            commands_context = context\n        )\n        speech_recognizer.delegate = voice_assistant\n        context.delegate = voice_assistant\n\n        main_task_group.soonify(speech_recognizer.start_listening)()\n        main_task_group.soonify(context.handle_responses)()\n\n        detector = BlockageDetector()\n        main_task_group.soonify(detector.monitor)()\n</code></pre> <p>Let's dissect it:</p> <pre><code>async def run(\n    manager: CommandsManager,\n    speech_recognizer: SpeechRecognizer,\n    speech_synthesizer: SpeechSynthesizer\n):\n</code></pre> <p>Parameters:</p> <ul> <li><code>manager</code>: An instance of <code>CommandsManager</code> which holds all the commands that the voice assistant can recognize and process.</li> <li><code>speech_recognizer</code>: The implementation you've selected for speech recognition.</li> <li><code>speech_synthesizer</code>: The implementation you've chosen for speech synthesis.</li> </ul> <pre><code>async with asyncer.create_task_group() as main_task_group:\n</code></pre> <p>Here, a task group is created using <code>asyncer</code>. Task groups allow you to manage several tasks concurrently.</p> <pre><code>context = CommandsContext(\n    task_group = main_task_group, \n    commands_manager = manager\n)\n</code></pre> <p>A <code>CommandsContext</code> is initialized. This holds the context in which commands are executed, including the associated task group and the command manager.</p> <pre><code>voice_assistant = VoiceAssistant(\n    speech_recognizer = speech_recognizer,\n    speech_synthesizer = speech_synthesizer,\n    commands_context = context\n)\n</code></pre> <p>The <code>VoiceAssistant</code> is then created and initialized with the recognizer, synthesizer, and context.</p> <pre><code>speech_recognizer.delegate = voice_assistant\ncontext.delegate = voice_assistant\n</code></pre> <p>Both the speech recognizer and the commands context are associated with the voice assistant as their delegates. This setup ensures that when the recognizer captures any speech or when there's a command response to handle, the voice assistant processes them.</p> <pre><code>main_task_group.soonify(speech_recognizer.start_listening)()\nmain_task_group.soonify(context.handle_responses)()\n</code></pre> <p>Tasks are added to the main task group: One to start the speech recognizer's listening process, and the other to handle responses from executed commands.</p> <pre><code>detector = BlockageDetector()\nmain_task_group.soonify(detector.monitor)()\n</code></pre> <p>A blockage detector is introduced and initialized. This mechanism ensures that any potential deadlocks or blocking calls within the async code are detected, allowing for smooth operation.</p>"},{"location":"advanced/custom-run/#customizing-the-run-function","title":"Customizing the Run Function","text":"<p>Customizing the <code>run</code> function provides a pathway to inject additional functionalities or to adapt the framework to specific needs.</p> <p>For instance, you could:</p> <ul> <li>Integrate other third-party tools or services.</li> <li>Implement custom logging or analytics mechanisms.</li> <li>Extend with other asynchronous operations to run concurrently with the voice assistant.</li> </ul> <p>When customizing, ensure that you maintain the core structure, especially the initialization of the main components and the task group management. The ordering can be crucial, especially when setting delegates.</p> <p>To kickstart your customization, replicate the default run function as your foundation, and weave in your specific adjustments or additions as needed. Consequently, a \"Hello, World\" implementation with a custom run would appear as:</p> <pre><code>import asyncer\nfrom stark import CommandsContext, CommandsManager, Response\nfrom stark.interfaces.protocols import SpeechRecognizer, SpeechSynthesizer\nfrom stark.interfaces.vosk import VoskSpeechRecognizer\nfrom stark.interfaces.silero import SileroSpeechSynthesizer\nfrom stark.voice_assistant import VoiceAssistant\nfrom stark.general.blockage_detector import BlockageDetector\n\n\nVOSK_MODEL_URL = \"YOUR_CHOSEN_VOSK_MODEL_URL\"\nSILERO_MODEL_URL = \"YOUR_CHOSEN_SILERO_MODEL_URL\"\n\nrecognizer = VoskSpeechRecognizer(model_url=VOSK_MODEL_URL)\nsynthesizer = SileroSpeechSynthesizer(model_url=SILERO_MODEL_URL)\n\nmanager = CommandsManager()\n\n@manager.new('hello')\nasync def hello_command() -&gt; Response:\n    text = voice = 'Hello, world!'\n    return Response(text=text, voice=voice)\n\nasync def run(\n    manager: CommandsManager,\n    speech_recognizer: SpeechRecognizer,\n    speech_synthesizer: SpeechSynthesizer\n):\n    async with asyncer.create_task_group() as main_task_group:\n        context = CommandsContext(\n            task_group = main_task_group, \n            commands_manager = manager\n        )\n        voice_assistant = VoiceAssistant(\n            speech_recognizer = speech_recognizer,\n            speech_synthesizer = speech_synthesizer,\n            commands_context = context\n        )\n        speech_recognizer.delegate = voice_assistant\n        context.delegate = voice_assistant\n\n        main_task_group.soonify(speech_recognizer.start_listening)()\n        main_task_group.soonify(context.handle_responses)()\n\n        detector = BlockageDetector()\n        main_task_group.soonify(detector.monitor)()\n\nasync def main():\n    await run(manager, recognizer, synthesizer)\n\nif __name__ == '__main__':\n    asyncer.runnify(main)() # or anyio.run(main), same thing\n</code></pre>"},{"location":"advanced/external-triggers/","title":"External Triggers","text":"<p>With the adaptability of Stark, VA can be integrated with various external triggers to provide a flexible and dynamic user experience. In the STARK framework, the integration of external triggers is seamless and can greatly enhance the interactivity of the assistant.</p> <p>In this guide, we will walk through how to set up and use external triggers to activate the STARK Voice Assistant.</p>"},{"location":"advanced/external-triggers/#setting-up-external-mode","title":"Setting Up External Mode","text":"<p>The STARK framework provides a dedicated mode for external triggers: the \"External\" mode. When you set the VA mode to \"external\", it waits for an explicit trigger to activate the <code>SpeechRecognizer</code> component.</p> <p>Additionally, you can utilize the <code>stop_after_interaction</code> property in custom modes:</p> <pre><code>stop_after_interaction=True\n</code></pre> <p>When set to <code>True</code>, this ensures that after the VA finishes its current interaction, it stops the <code>SpeechRecognizer</code>, allowing for the next interaction to be initiated by an external trigger.</p> <p>Details on the Voice Assistant page.</p>"},{"location":"advanced/external-triggers/#triggering-using-start_listening","title":"Triggering Using <code>start_listening()</code>","text":"<p>Once the VA has stopped listening after an interaction, you can restart the <code>SpeechRecognizer</code> using the <code>start_listening()</code> method. This method serves as an entry point when you want to reactivate voice recognition after an external trigger.</p>"},{"location":"advanced/external-triggers/#implementing-external-triggers","title":"Implementing External Triggers","text":"<p>Do note that you probably need to implement a custom run function to add cuncurrent process or create a separate thread.</p> <p>The beauty of external triggers lies in their versatility. Here are some ways to integrate them:</p>"},{"location":"advanced/external-triggers/#keyboard-hotkey-shortcut","title":"Keyboard Hotkey Shortcut","text":"<p>A simple approach is to have a specific keyboard combination to activate Stark. Tools like Python's <code>keyboard</code> library can help in detecting specific keypresses, enabling you to then call <code>start_listening()</code>.</p>"},{"location":"advanced/external-triggers/#hardware-integration","title":"Hardware Integration","text":"<p>For those looking for a hands-free approach, integrating hardware can be a fascinating option. For instance, using an Arduino microphone module, you can set up a system where Stark activates upon a distinct sound pattern, like a double or triple clap.</p>"},{"location":"advanced/external-triggers/#fast-wakeword-detectors","title":"Fast Wakeword Detectors","text":"<p>Wakeword detection is a popular approach in modern VAs. Using fast lightweight wakeword detectors like Picovoice's Porcupine, you can have your VA spring into action upon hearing a specific keyword or phrase.</p>"},{"location":"advanced/external-triggers/#implementations-and-examples","title":"Implementations and Examples","text":"<p>You can find external trigger implementations at stark_place/triggers and examples of usage at stark_place/examples.</p> <p>By embracing external triggers, you can elevate the adaptability and user experience of your voice assistant. Whether it's a simple keyboard shortcut or an intricate hardware setup, STARK's flexibility ensures that your VA is always ready and responsive, aligned with the needs of your user base.</p>"},{"location":"advanced/fallback-command-llm-integration/","title":"Fallback Command / LLM Integration","text":"<p>In the dynamic world of voice assistants and speech recognition, it's essential to account for the unpredictability of user input. Despite the comprehensive list of commands you may have configured, there will inevitably be instances where user utterances don't align with any predefined command. This is where the fallback command comes in.</p> <p>The fallback command in the STARK framework serves as a safety net, ensuring that when a user's voice input doesn't match any set command, there's still an appropriate and meaningful response.</p>"},{"location":"advanced/fallback-command-llm-integration/#setting-up-the-fallback-command","title":"Setting Up the Fallback Command","text":"<p>In the STARK framework, integrating a fallback command is streamlined. You can assign the <code>fallback_command</code> to the <code>CommandsContext</code> directly:</p> <pre><code>CommandsContext.fallback_command: Command\n</code></pre> <p>Here's a practical example:</p> <pre><code>from stark.core.types import String\n...\n\n@manager.new('$string:String', hidden=True)\nasync def fallback(string: String):\n    # Your fallback logic here\n    ...\n\ncommands_context.fallback_command = fallback\n</code></pre> <p>In this example, any unrecognized string is directed to the <code>fallback</code> function, allowing you to define how the system should respond.</p>"},{"location":"advanced/fallback-command-llm-integration/#fallback-command-options","title":"Fallback Command Options","text":"<p>With the rise of advanced language models like ChatGPT, it's now feasible to provide intelligent and contextually relevant responses even for unexpected user inputs. Integrating an LLM can elevate the user experience, making your voice assistant appear more intuitive and responsive.</p> <p>Fallbacks aren't limited to LLMs. You can get creative with your approach. Consider these options:</p> <ul> <li>Wikipedia API: Search for a quick answer or definition related to the user's query.</li> <li>Google Search Parsing: Extract snippets from top search results for a quick response.</li> <li>Custom Database Lookups: If you have a specific dataset or database, direct fallback queries there.</li> <li>Fun random \"I don't know\" synonyms</li> </ul> <p>Fallback commands are invaluable, ensuring your voice assistant remains responsive, intelligent, and user-friendly, even in the face of unexpected inputs. With the flexibility of STARK and the power of modern Large Language Models, creating a robust voice assistant has never been easier.</p>"},{"location":"advanced/localization-and-multi-language/","title":"Localization and Multi-Language","text":"<p>Expecting Soon...</p> <p>In the meantime, if you're eager to contribute and expedite the growth of STARK, consider checking out the STARK PLACE. By contributing to the library, you can play a pivotal role in implementing features more rapidly and enhancing the framework for everyone.</p> <p>\ud83d\udd17 Interested in contributing? Dive into our Contribution and Shared Usage Guidelines for all the details!</p>"},{"location":"advanced/optimization/","title":"Optimization for Stark","text":"<p>When it comes to Stark, or any software platform, optimization is pivotal to ensuring smooth and efficient operations. Here are some pivotal guidelines and best practices to ensure that Stark runs at its best:</p>"},{"location":"advanced/optimization/#non-blocking-is-key","title":"Non-blocking is Key","text":"<p>THE MOST IMPORTANT: Always ensure that you DO NOT place blocking code inside <code>async def</code> functions. Blocking code can drastically reduce the performance of asynchronous applications by halting the execution of other parts of the application.</p> <p>If you have commands that run blocking code, always define them using the simple <code>def</code> (Sync-vs-Async). This ensures that Stark creates a separate worker thread to handle the execution of that command. By doing so, Stark remains responsive, even when processing resource-intensive commands.</p>"},{"location":"advanced/optimization/#sync-vs-async","title":"Sync vs Async","text":"<p>Understanding the difference between synchronous and asynchronous code is crucial. Asynchronous code allows your application to perform other tasks while waiting for a particular task to complete, thus improving efficiency. The Sync-vs-Async page provides a comprehensive comparison and guidance on how to effectively leverage both.</p>"},{"location":"advanced/optimization/#utilizing-the-asyncer","title":"Utilizing the asyncer","text":"<p>The asyncer documentation is a valuable resource. It provides an array of tools and methods to help convert synchronous code to asynchronous and vice-versa, aiding in the optimization process.</p>"},{"location":"advanced/optimization/#using-asyncerasyncify","title":"Using asyncer.asyncify","text":"<p>If you need to call blocking synchronous code within an <code>async def</code> function, utilize <code>asyncer.asyncify</code>. It allows you to effectively run synchronous code inside an asynchronous function without blocking the entire event loop.</p>"},{"location":"advanced/optimization/#grouping-asynchronous-requests","title":"Grouping Asynchronous Requests","text":"<p>If you have multiple asynchronous tasks that can be executed concurrently, group them together and await them as one unit. This approach allows tasks to be run simultaneously, improving the overall speed of the function.</p> <pre><code>async def task_one():\n    ...\n\nasync def task_two():\n    ...\n\n# or\nimport anyio\nasync with anyio.create_task_group() as task_group:\n    task_group.start_soon(task_one)\n    task_group.start_soon(task_two)\n# or\nimport asyncer\nasync with asyncer.create_task_group() as task_group\n    task_group.soonify(task_one)()\n    task_group.soonify(task_one)()\n# or \nimport asyncio\nawait asyncio.gather(task_one(), task_two())\n</code></pre>"},{"location":"advanced/optimization/#implement-caching","title":"Implement Caching","text":"<p>Caching is a practice of storing frequently used data or results in a location for quicker access in the future. By implementing caching, you can significantly reduce repetitive computations and database lookups, leading to faster response times. Python libraries like <code>cachetools</code> or <code>functools.lru_cache</code> are popular tools for caching.</p> <p>Optimization is a continuous process. As Stark grows and evolves, always look out for opportunities to refine and streamline its operations. Remember, the key is to ensure Stark remains responsive and efficient, offering users a seamless and efficient voice assistant experience.</p>"},{"location":"advanced/other/","title":"Advanced Exploration","text":"<p>Congratulations on navigating through the entirety of the STARK documentation! We've aimed to cover a comprehensive range of topics and scenarios to help you get the most out of the framework. However, the vast expanse of technology means there might always be nuances or specific use cases we might not have touched upon.</p>"},{"location":"advanced/other/#delving-deeper","title":"Delving Deeper","text":"<p>If you've scoured the documentation and still haven't found the precise information you're looking for, consider the following resources:</p> <ul> <li> <p>Source Code: Often, the code itself and it's tests can be the best documentation. Delve into the inner workings and intricacies of the STARK framework by perusing the source code.</p> </li> <li> <p>Issues &amp; Discussions: Engage with the community and the developers. The Issues and Discussions sections can offer insights into known challenges, proposed enhancements, and community-contributed solutions.</p> </li> <li> <p>STARK PLACE Repository: Apart from the main repository, the STARK PLACE repo houses a plethora of shared modules, extensions, and utilities. It's an excellent place to find (or contribute) additional tools or modules that might be relevant to your needs.</p> </li> </ul>"},{"location":"advanced/other/#customization-and-extension","title":"Customization and Extension","text":"<p>STARK is designed with flexibility at its core. If you come across a situation where the existing methods don't align perfectly with your requirements, remember:</p> <ul> <li>Subclassing: Feel empowered to subclass any module within the STARK framework. By doing so, you can maintain the foundational behavior and only override the specific methods you need to tailor to your needs.</li> </ul> <p>Should you choose to delve into the source, customize components, or even contribute to the repository, we're excited to have you on board, pushing the boundaries of what STARK can achieve. Here's to building, innovating, and advancing together!</p>"},{"location":"tools/phonetic-dictionary/","title":"Dictionary - Phonetic Lookup","text":"<p>NOTE: requires libespeak-ng binary installed in the system</p>"},{"location":"tools/phonetic-dictionary/#overview","title":"Overview","text":""},{"location":"tools/phonetic-dictionary/#basic-lookup","title":"Basic Lookup","text":"<p>Create a dictionary in memory and add an entry: <pre><code>dictionary = Dictionary(storage=DictionaryStorageMemory())\ndictionary.write_one('en', \"Linkin Park\", {\"id\": 1234})\n</code></pre></p> <p>Then you can look up names by different spellings, homophones, or even cross-language phonetic similarity: <pre><code>matches = dictionary.lookup(\"linkoln perk\", 'en')\nmatches[0].metadata  # {\"id\": 1234})\n\nmatches = dictionary.lookup(\"\u043b\u0456\u043d\u043a\u0456\u043d \u043f\u0430\u0440\u043a\", 'ua') # ukrainian spelling of Linkin Park\nmatches[0].metadata  # {\"id\": 1234})\n</code></pre></p>"},{"location":"tools/phonetic-dictionary/#search-in-sentence","title":"Search in Sentence","text":"<p>You can also scan an entire sentence for names from a dictionary:</p> <pre><code>dictionary.search_in_sentence(\"good morning play linkin park on spotify\", 'en')\n</code></pre> <p>Both <code>lookup</code> and <code>search_in_sentence</code> receive two optional parameters: <code>mode: LookupMode = .AUTO</code> and <code>field: LookupField = .PHONETIC</code>.</p> <pre><code>class LookupMode(Enum):\n    EXACT = auto()  # the fastest\n    CONTAINS = auto()  # fast\n    FUZZY = auto()  # slow, not recommended at 10K+ entries\n    AUTO = auto()  # recommended: tries modes sequentially until match with some dict-size limits\n\nclass LookupField(Enum):\n    NAME = auto()  # search by original name, only same lang is reasonable\n    PHONETIC = auto()  # search by phonetic similarity, cross-lang support\n</code></pre>"},{"location":"tools/phonetic-dictionary/#sorting","title":"Sorting","text":"<p>Also, there are <code>lookup_sorted</code> and <code>search_in_sentence_sorted</code> methods that automatically sort results by levenshtein distance. These might add a noticeable overhead when many entries are matched (starting from magnitude of a hundred). In most cases, it's better to use the not sorted version, check results amount, and then sort them manually if needed. Example of levenshtein sort:</p> <pre><code>sorted(\n    matches,\n    key=lambda item: levenshtein_similarity( # sort by the original name for same languages\n        s1=name_candidate,\n        s2=item.name,\n    ) if item.language_code == language_code else levenshtein_similarity( # sort by phonetic similarity for cross-language\n        s1=transcription(name_candidate, language_code),\n        s2=item.phonetic,\n    ),\n    reverse=True,\n)\n</code></pre> <p>More details about levenshtein for fuzzy string matching here page.</p> <p>But in many cases, domain-specific sorting and filtering is the best approach. For example, in navigator app you can prioritize names that are closer to the user's location. For example, Georgia the state for american users, but Georgia the country for european.</p>"},{"location":"tools/phonetic-dictionary/#using-with-nldictionaryname","title":"Using with NLDictionaryName","text":"<p>You can use NLDictionaryName to parse and match names from a Dictionary. It has already implemented <code>did_parse</code>, so no need to implement it yourself.</p> <pre><code>from stark.tools.dictionary.dictionary import Dictionary\nfrom stark.tools.dictionary.nl_dictionary_name import NLDictionaryName\nfrom stark.tools.dictionary.storage import DictionaryStorageMemory\n\nclass NLCityName(NLDictionaryName):\n    dictionary = Dictionary(storage=DictionaryStorageMemory()) # any NLDictionaryName must implement dictionary: Dictionary\n\n# Fill the dictionary as usual\nNLCityName.dictionary.clear()\nNLCityName.dictionary.write_one(\"de\", \"N\u00fcrnberg\", {\"coords\": (49.45, 11.08)})\nNLCityName.dictionary.write_one(\"en\", \"London\",   {\"coords\": (51.51, -0.13)})\nNLCityName.dictionary.write_one(\"en\", \"Paris\",    {\"coords\": (48.85, 2.35 )})\n\n@manager.new('weather in $city:NLCityName')\ndef hello(weather: NLCityName):\n    print(weather.value[0].item.metadata[\"coords\"]) # (48.85, 2.35) for \"weather in parish\"\n</code></pre> <p>Data model overview:</p> <pre><code>class NLDictionaryName:\n    value: list[LookupResult]\n    dictionary: Dictionary\n\nclass LookupResult:\n    span: Span\n    item: DictionaryItem\n\n@dataclass\nclass DictionaryItem:\n    name: str\n    phonetic: str\n    simple_phonetic: str\n    language_code: str\n    metadata: Metadata # dict[str, object]\n</code></pre> <p>Inspect your IDE suggestions and the source code (most modern editors support \"go to definition\" feature) for more details.</p>"},{"location":"tools/phonetic-dictionary/#encapsulate-storage-and-filling-logic","title":"Encapsulate Storage and Filling Logic","text":"<p>You can encapsulate storage and filling logic in a single class:</p> <pre><code>class MyDictionary(Dictionary):\n    def __init__(self):\n        super().__init__(storage=DictionaryStorageSQL(\"sqlite:///my-phonetic-dictionary.db\"))\n\n    async def build(self):\n        self.write_all(...)  # Fill from files, db, or API\n\nclass NLExampleDictionaryName(NLObject):\n    dictionary = MyDictionary()\n</code></pre>"},{"location":"tools/phonetic-dictionary/#building-example","title":"Building Example","text":"<p>While you can modify a Dictionary even in runtime, the best approach is to fill the dictionary at build stage if possible, since writing might be slow for large dictionaries (especially starting from magnitudes of thousands). There is the example main.py that uses typer to add <code>build</code> and <code>run</code> cli commands to your app.</p> <pre><code>import typer\n\ncli = typer.Typer()\n\n@cli.command()\ndef build():\n    \"\"\"Build the project. See typer docs for better CLI with features like progress bars and logging.\"\"\"\n    print(\"Building...\")\n    NLExampleDictionaryName.dictionary.build_if_needed() # fill the sqlite file once during the build stage, not at runtime\n    SomeOtherDictionary.build() # or force re-build on each call\n    # etc\n    print(\"Done\")\n\n\n@cli.command()\ndef run():\n    \"\"\"Run your main app here.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\":\n    cli()\n</code></pre>"},{"location":"tools/raw-phonetic/","title":"Raw Phonetic Tools: IPA &amp; Simplephone","text":""},{"location":"tools/raw-phonetic/#overview","title":"Overview","text":"<p>These tools convert text to phonetic representations for fuzzy matching, name lookup, and cross-language search. They power the phonetic matching in the Dictionary Tool and are often used together (simplephone code of the phonetic transcription) for best results.</p> <ul> <li><code>transcription</code>: Converts text in any language to a simplified Latin transcription using IPA (International Phonetic Alphabet). The default implementation currently uses espeak-ng (requires libespeak-ng binary installed in the system). STARK also provides an epitran wrapper as an alternative for espeak, and allows passing any custom implementation as a parameter.</li> <li><code>simplephone</code>: Further reduces a transcription (or plain English text) to a simple, language-agnostic phonetic code for fast, robust matching.</li> </ul>"},{"location":"tools/raw-phonetic/#basic-usage","title":"Basic Usage","text":"<pre><code>from stark.tools.phonetic.transcription import transcription, ipa2lat\nfrom stark.tools.phonetic.simplephone import simplephone\n\n# Convert ukrainian to simplified Latin phonetic transcription (IPA-based)\nipa = transcription(\"\u041b\u0456\u043d\u043a\u0456\u043d \u041f\u0430\u0440\u043a\", \"uk\")  # e.g. \"\u041b\u0456\u043d\u043a\u0456\u043d \u041f\u0430\u0440\u043a\" \u2192 \"linkin park\"\n\n# Convert to simplephone code (robust, language-agnostic)\nsp = simplephone(\"Linkin Park\")      # e.g. \"linkin park\" \u2192 \"LNKNPARK\"\n\n# Combine for best fuzzy matching (recommended for cross-language)\nsp_combined = simplephone(transcription(\"\u041b\u0456\u043d\u043a\u0456\u043d \u041f\u0430\u0440\u043a\", \"uk\"))  # \u2192 \"LNKNPARK\"\n\n# Direct IPA to Latin conversion\nlatin = ipa2lat(\"t\u025bst\")  # \u2192 \"test\"\n</code></pre>"},{"location":"tools/raw-phonetic/#function-reference","title":"Function Reference","text":""},{"location":"tools/raw-phonetic/#def-transcription","title":"def transcription","text":"<pre><code>def transcription(text: str, language_code: str, ipa_provider: IpaProvider = EspeakIpaProvider()) -&gt; str\n</code></pre> <ul> <li>Converts a string to a simplified Latin phonetic transcription using IPA via espeak-ng.</li> <li>Handles many languages (see espeak-ng docs for supported codes).</li> <li>Used for cross-language and accent-insensitive matching.</li> </ul> <p>Parameters: - <code>text</code>: Input string. - <code>language_code</code>: BCP-47 or ISO language code (e.g. <code>\"en\"</code>, <code>\"uk\"</code>, <code>\"de\"</code>). - <code>ipa_provider</code>: Optional, allows custom IPA provider (default: EspeakIpaProvider).</p> <p>Returns: Simplified Latin transcription as a string.</p>"},{"location":"tools/raw-phonetic/#def-simplephone","title":"def simplephone","text":"<pre><code>def simplephone(text: str, glue: str = \" \", sep: str = string.whitespace) -&gt; str | None\n</code></pre> <ul> <li>Converts a string to a simple, language-agnostic phonetic code.</li> <li>Inspired by Caverphone, Soundex, and K\u00f6lner Phonetik.</li> <li>Ignores spaces, strips non-alphabetic characters, and normalizes similar sounds.</li> </ul> <p>Parameters: - <code>text</code>: Input string consisting of latin characters. - <code>glue</code>: Separator for joining words (default: space). - <code>sep</code>: Characters to treat as word separators (default: whitespace).</p> <p>Returns: Simplephone code as a string, or <code>None</code> if input is empty.</p>"},{"location":"tools/raw-phonetic/#typical-usage-pattern","title":"Typical Usage Pattern","text":"<p>For best fuzzy matching (especially cross-language), use both together:</p> <pre><code># For English input\na = simplephone(transcription(\"Linkin Park\", \"en\"))  # \u2192 \"LNKNPARK\"\n\n# For Ukrainian input\nb = simplephone(transcription(\"\u041b\u0456\u043d\u043a\u0456\u043d \u041f\u0430\u0440\u043a\", \"uk\"))  # \u2192 \"LNKNPARK\"\n\na == b # True\n</code></pre> <p>This enables matching names and words across different languages and spellings.</p>"},{"location":"tools/raw-phonetic/#more-fuzzyness","title":"More fuzzyness","text":"<p>For even more fuzzyness, consider using the levenshtein distance with the default proximity graph for simplephone (<code>SIMPLEPHONE_PROXIMITY_GRAPH</code>). For details see STARK's Levenshtein implementation</p>"},{"location":"tools/raw-phonetic/#notes","title":"Notes","text":"<ul> <li><code>transcription</code> requires espeak-ng installed on your system.</li> <li>These functions are used internally by the Dictionary Tool for phonetic and fuzzy lookup.</li> <li>For more details, see the source code or use your IDE's autocomplete.</li> </ul>"},{"location":"tools/sliding-window-parser/","title":"Sliding Window Parser","text":""},{"location":"tools/sliding-window-parser/#overview","title":"Overview","text":"<p><code>sliding_window_parser</code> helps you find and extract parameters from free text using a parser function even if it doesn't parse the entire input or returns just the value without the substring or the span. It slides through the sentence with growing/shrinking substring windows and tests each span until finds a suitable match.</p>"},{"location":"tools/sliding-window-parser/#basic-usage","title":"Basic Usage","text":"<pre><code>from stark.tools.sliding_window_parser import sliding_window_parse, Span\n\nasync def date_parser(text: str):\n    if text.lower() in {\"september 5\", \"5 september\"}:\n        return (\"date\", \"2024-09-05\")\n    if text.lower() == \"september\":\n        return (\"month\", \"09\")\n    return None\n\nresult = await sliding_window_parse(\n    \"remind me to call mom on september 5\",\n    parser=date_parser,\n)\nprint(result) # [(Span(27, 39), \"september 5\", (\"date\", \"2024-09-05\"))]\n</code></pre>"},{"location":"tools/sliding-window-parser/#parameters","title":"Parameters","text":"<pre><code>async def sliding_window_parse(\n    phrase: str,\n    parser: Callable[[str], Awaitable[T]],\n    min_window: int = 1,\n    max_window: int | None = None,\n    concurrency: int | None = None,\n    find_one: bool = True,\n) -&gt; list[tuple[Span, str, T]]:\n\n- **phrase** \u2013 text to parse\n- **parser** \u2013 async callable returning a parsed value, `None`, or ParseError\n- **min_window / max_window** \u2013 window size range in tokens (words)\n- **concurrency** \u2013 limit parallel parser calls, default is `None` (unlimited)\n- **find_one** \u2013 stop after first match instead of collecting all\n\nReturns:\n        A list of tuples (span, substring, value) for each match, where:\n        - span: Span object with character offsets (start, end) in the original phrase\n        - substring: the matched substring (phrase[span.start:span.end])\n        - value: the value returned by the parser\n\n        If find_one=True, returns a single-item list with the first match (faster, less parser calls).\n        If no match is found, raises ParseError, so the list is never empty, meaning result[0] is always safe.\n</code></pre>"},{"location":"tools/stark-levenshtein/","title":"STARK-Levenshtein - Fuzzy String Matching","text":""},{"location":"tools/stark-levenshtein/#overview","title":"Overview","text":"<p>Minimal wrappers for Levenshtein distance and similarity, with optional phonetic/character proximity graphs, substring search, and prefix/suffix ignoring. Useful for fuzzy string matching, similarity scoring, and fuzzy substring search. Written in cython and compiled for performance.</p>"},{"location":"tools/stark-levenshtein/#basic-usage","title":"Basic Usage","text":"<pre><code>from stark.tools.levenshtein import (\n    levenshtein_distance,\n    levenshtein_similarity,\n    levenshtein_match,\n    levenshtein_distance_substring,\n    levenshtein_search_substring,\n    SIMPLEPHONE_PROXIMITY_GRAPH, # Is more meaningful to use for simplephone strings, see phonetic tools docs\n    SKIP_SPACES_GRAPH, # ignores spaces while matching\n)\n\n# Get the Levenshtein distance (lower = more similar, 0 = exact match)\nlev = levenshtein_distance(s1=\"kitten\", s2=\"sitting\")\n\n# Get similarity score (0.0 to 1.0, higher = more similar)\nsim = levenshtein_similarity(s1=\"kitten\", s2=\"sitting\")\n\n# Check if two strings are similar enough (similarity &gt;= threshold)\nis_match = levenshtein_match(s1=\"kitten\", s2=\"sitting\", threshold=0.7)\n\n# Find all substrings in s2 with minimal distance to s1\ndist_spans = levenshtein_distance_substring(s1=\"kitten\", s2=\"the sitting cat\")\n# Returns: list of (Span, distance)\n\n# Find substrings in s2 where similarity to s1 is above threshold\nsearch_spans = levenshtein_search_substring(s1=\"kitten\", s2=\"the sitting cat\", threshold=0.7)\n# Returns: list of (Span, similarity)\n</code></pre>"},{"location":"tools/stark-levenshtein/#parameters","title":"Parameters","text":"<p>All functions accept:</p> <ul> <li><code>s1: str</code> \u2013 first string to compare (required)</li> <li><code>s2: str</code> \u2013 second string to compare (required)</li> <li><code>proximity_graph: dict[str, dict[str, float]] | None = None</code> \u2013 custom operation costs instead of default 1. For example, based on phonetic similarity, keyboard proximity, or just to ignore some characters.</li> <li><code>max_distance: float | None = None</code> \u2013 skip calculation if distance exceeds this value and early_return is True (optional)</li> <li><code>ignore_prefix: bool = False</code> \u2013 ignore matching prefixes, required for substring search</li> <li><code>ignore_suffix: bool = False</code> \u2013 ignore matching suffixes, breaks substring search</li> <li><code>narrow: bool = False</code> \u2013 restrict to shortest possible substring (substring search)</li> <li><code>early_return: bool = True</code> \u2013 return as soon as threshold is met (faster). False value is for debug only.</li> <li><code>lower: bool = False</code> \u2013 compare strings as lowercase</li> </ul> <p>Functions with a <code>threshold</code> parameter: - <code>threshold: float = 0</code> \u2013 similarity threshold for match/search; used to calc max_distance, which stops the calculation early if distance exceeds this value to improve performance</p>"},{"location":"tools/stark-levenshtein/#constants","title":"Constants","text":"<pre><code>type ProximityGraph = dict[str, dict[str, float]]\nPROX_MED = 0.5\nPROX_LOW = 0.25\nPROX_MIN = 0.01\nSIMPLEPHONE_PROXIMITY_GRAPH: ProximityGraph = {\n    \"w\": {\"f\": PROX_MED, \"a\": PROX_LOW, \"y\": PROX_LOW},\n    \"y\": {\"a\": PROX_LOW, \"w\": PROX_LOW},\n    \"a\": {\"y\": PROX_LOW, \"w\": PROX_LOW, \"-\": PROX_LOW},  # '-' for deletion\n    \"f\": {\"w\": PROX_MED},\n    \" \": {\"-\": PROX_MIN},  # ignore spaces\n    \"-\": {\"a\": PROX_LOW, \" \": PROX_MIN},  # insertion\n}\nSKIP_SPACES_GRAPH = {\" \": {\"-\": PROX_MIN}, \"-\": {\" \": PROX_MIN}}\n</code></pre> <p>For more advanced usage, see the source code or use your IDE's autocomplete.</p>"}]}